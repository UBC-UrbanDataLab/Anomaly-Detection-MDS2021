{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of the test env code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import files from sibling a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "\n",
    "import model.clean as cl\n",
    "import model.model_trainer as mt\n",
    "import model.model_predict as mp\n",
    "from model.influx_interact import influx_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Set up a local instance of influx by following the README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2  \n",
    "`populate_influx.py`  \n",
    "Populating influx with data from csvs stored in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "CSVS_TO_LOAD = [\n",
    "    \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"CEC_compiled_data_5b_updated.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up influx connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as set up in the docker-compose\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "\n",
    "# set up influx\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, timeout=999_000)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each csv, read it then write it into influx  \n",
    "\n",
    "Important note: If the influx write times out, re-run and it should work on second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: CEC_compiled_data_1b_updated.csv\n",
      "writing: CEC_compiled_data_2b_updated.csv\n",
      "writing: CEC_compiled_data_3b_updated.csv\n",
      "writing: CEC_compiled_data_4b_updated.csv\n",
      "writing: CEC_compiled_data_5b_updated.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in CSVS_TO_LOAD:\n",
    "\n",
    "    # load and set up dataframes\n",
    "    df = pd.read_csv(PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"])\n",
    "    df.rename(columns={\"Value\": \"val_num\"}, inplace=True)\n",
    "    df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    df.rename(columns={\"Anomaly\": \"AH\"}, inplace=True)\n",
    "    df[\"navName\"] = \"Energy\"\n",
    "    df[\"siteRef\"] = \"Campus Energy Centre\"\n",
    "    df.set_index(\"Datetime\", drop=True, inplace=True)\n",
    "    df = df.drop([\"AH\"], axis=1)\n",
    "\n",
    "    print(\"writing: {}\".format(csv))\n",
    "    # write values\n",
    "    write_api.write(\n",
    "        bucket,\n",
    "        org,\n",
    "        record=df,\n",
    "        data_frame_measurement_name=\"READINGS\",\n",
    "        data_frame_tag_columns=[\"uniqueID\", \"navName\", \"siteRef\"],\n",
    "    )\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `df` object to see what was written to influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_num</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>navName</th>\n",
       "      <th>siteRef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 07:45:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:00:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:15:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:30:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:45:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     val_num                                    uniqueID  \\\n",
       "Datetime                                                                   \n",
       "2020-01-01 07:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:00:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:15:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:30:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "\n",
       "                    navName               siteRef  \n",
       "Datetime                                           \n",
       "2020-01-01 07:45:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:00:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:15:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:30:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:45:00  Energy  Campus Energy Centre  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sensor data has been written to influx's READINGS bucket  \n",
    "This simulates what UDL's influx will look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "`test_env_scheduled_training.py`  \n",
    "Run the training file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the training data for fast dev iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup thresholds for anomaly detection.  \n",
    "If a training example's loss is greater than the set threshold it will be flagged as anomalous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\": 0.09,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\": 0.019,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\": 0.0725,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\": 0.02938,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\": 0.043,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a end time for the readings to be used for training  \n",
    "In the real implementation there will be no end_time for the training data, i.e it will train on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = 1613109600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore training with removing anomalous records, i.e only train on normal data for this specific sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ANOMALOUS = True\n",
    "REMOVE_ANOMALOUS_DATA = [\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Influx connector setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./test_env_models/\"\n",
    "scaler_path = \"./test_env_standardizers/\"\n",
    "\n",
    "# set up for influx\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "url = \"http://localhost:8086\"\n",
    "\n",
    "influx_conn = influx_class(\n",
    "    org=org,\n",
    "    url=url,\n",
    "    bucket=bucket,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training data from influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on sensor ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `main_bucket` object is a dict with the name of the sensor as the key and then the value is another dict of data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Campus Energy Centre Boiler B-1 Exhaust O2', 'Campus Energy Centre Boiler B-1 Gas Pressure', 'Campus Energy Centre Campus HW Main Meter Entering Water Temperature', 'Campus Energy Centre Campus HW Main Meter Flow', 'Campus Energy Centre Campus HW Main Meter Power'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_bucket.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell for training:\n",
    "1) Iterates over each the data sets (line 1)  \n",
    "2) Removes anomalous data if the data set has been specified (lines 4-23)  \n",
    "3) Standardizes the values for training and saves the standardizer for the prediction step (lines 25-30)  \n",
    "4) Subsets the data for faster training if specified (lines 32-33)  \n",
    "5) Sequences the values into windows for the LSTM (lines 35-37)  \n",
    "6) Fits and saves the model (line 42)  \n",
    "7) Writes the resulting data (that contains: value, anomalies detected manually, and anomalies detected in training) to influx (lines 44-57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for : Campus Energy Centre Boiler B-1 Exhaust O2\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 6s 23ms/step - loss: 0.0010 - val_loss: 1.4035\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 3.5567e-04 - val_loss: 1.4194\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 4.2476e-04 - val_loss: 1.4245\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 1.1841e-04 - val_loss: 1.4251\n",
      "Training for : Campus Energy Centre Boiler B-1 Gas Pressure\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 6s 24ms/step - loss: 0.2650 - val_loss: 0.3818\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 2s 18ms/step - loss: 0.2121 - val_loss: 0.3204\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.2086 - val_loss: 0.3192\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2052 - val_loss: 0.3081\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.2032 - val_loss: 0.3544\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.2037 - val_loss: 0.3760\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2019 - val_loss: 0.3891\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Entering Water Temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "141/141 [==============================] - 6s 22ms/step - loss: 0.2807 - val_loss: 0.3015\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1897 - val_loss: 0.2515\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1589 - val_loss: 0.1586\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1516 - val_loss: 0.1408\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1418 - val_loss: 0.1738\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1393 - val_loss: 0.1361\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1366 - val_loss: 0.1177\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1308 - val_loss: 0.1178\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1306 - val_loss: 0.1237\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1266 - val_loss: 0.1228\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Flow\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 7s 27ms/step - loss: 0.2952 - val_loss: 0.1233\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.2732 - val_loss: 0.2241\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.2373 - val_loss: 0.3879\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2376 - val_loss: 0.4466\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Power\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 7s 25ms/step - loss: 0.2878 - val_loss: 0.2326\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.2458 - val_loss: 0.2230\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2118 - val_loss: 0.2015\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1951 - val_loss: 0.1745\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1916 - val_loss: 0.1668\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1877 - val_loss: 0.1624\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1849 - val_loss: 0.1598\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1809 - val_loss: 0.1521\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1802 - val_loss: 0.1458\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1744 - val_loss: 0.1423\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1730 - val_loss: 0.1391\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1692 - val_loss: 0.1351\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1651 - val_loss: 0.1365\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1655 - val_loss: 0.1365\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1661 - val_loss: 0.1352\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    print(\"Training for : {}\".format(key))\n",
    "\n",
    "    # removes anomalies to only train on normal data\n",
    "    if REMOVE_ANOMALOUS:\n",
    "        if key in REMOVE_ANOMALOUS_DATA:\n",
    "            PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "            csv = \"CEC_compiled_data_2b_updated.csv\"\n",
    "            df_with_manual_anomaly = pd.read_csv(\n",
    "                PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"]\n",
    "            )\n",
    "            df_with_manual_anomaly[\"Datetime\"] = pd.to_datetime(\n",
    "                df_with_manual_anomaly[\"Datetime\"], utc=True\n",
    "            )\n",
    "            df = df.merge(\n",
    "                df_with_manual_anomaly[[\"Datetime\", \"Anomaly\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"DateTime\",\n",
    "                right_on=\"Datetime\",\n",
    "            )\n",
    "            df = df.loc[df[\"Anomaly\"] == False]\n",
    "            df = df.drop(columns=[\"DateTime\"], axis=1)\n",
    "            am_df.rename(columns={\"Anomaly\": \"manual_anomaly\"}, inplace=True)\n",
    "\n",
    "    # creates standardized column for each sensor in main bucket\n",
    "    df[\"Stand_Val\"] = cl.std_val_train(\n",
    "        df[[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    if TESTING:\n",
    "        df = df.tail(5000)\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"])\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    normal_dict = cl.model_parser(df, x_train, y_train)\n",
    "\n",
    "    threshold = THRESHOLDS[key]\n",
    "    mt.fit_models(normal_dict, model_path, threshold)\n",
    "\n",
    "    # for writing AM to influx\n",
    "    am_df = normal_dict[key][\"train_score_df\"]\n",
    "    am_df.rename(columns={\"anomaly\": \"model_anomaly\"}, inplace=True)\n",
    "    am_df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    am_df.rename(columns={\"Datetime\": \"DateTime\"}, inplace=True)\n",
    "    am_df[\"val_num\"] = df[\"Value\"].iloc[x_train.shape[1] :]\n",
    "    # only if it hasnt already been created earlier\n",
    "    if \"manual_anomaly\" not in set(am_df.columns):\n",
    "        am_df[\"manual_anomaly\"] = False\n",
    "    am_df.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "    am_df = am_df[[\"uniqueID\", \"model_anomaly\", \"val_num\", \"manual_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(am_df, \"TRAINING_ANOMALY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "`test_env_scheduled_predictor.py`  \n",
    "Create predictions and write to influx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up start and end times for the prediction data set in the test env.  \n",
    "In the real implementation END_TIME would be `now()` and START_TIME would be `now() - 1d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TIME FOR TRAINING SET BECOMES PREDICTING'S START TIME\n",
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data to be predicted on from influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from influx\n"
     ]
    }
   ],
   "source": [
    "print(\"reading data from influx\")\n",
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split based on data name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell for predicting:\n",
    "1) Iterates over each the data sets (line 1)  \n",
    "2) Standardizes the values for training by loading the standardizer from the training step (lines 2-6)     \n",
    "3) Sequences the values into windows for the LSTM and other reshaping for the prediction step(lines 8-14)  \n",
    "4) Creates predictions for the data and returns the prediction object (lines 16-24)   \n",
    "5) Shapes the prediction object for writing back to influx (a dataframe that contains, value, and the prediction of anomalous) (lines 26-34) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    main_bucket[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket[key][[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    x_train, y_train = mt.create_sequences(\n",
    "        main_bucket[key][\"Stand_Val\"], main_bucket[key][\"Stand_Val\"]\n",
    "    )\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    threshold = THRESHOLDS[key]\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        model_path,\n",
    "    )\n",
    "    ar_df = pd.DataFrame.from_dict(pred[\"data\"])\n",
    "\n",
    "    # prep for writing\n",
    "    ar_df.rename(columns={\"anomaly\": \"realtime_anomaly\"}, inplace=True)\n",
    "    ar_df.rename(columns={\"Timestamp\": \"DateTime\"}, inplace=True)\n",
    "    ar_df[\"uniqueID\"] = key\n",
    "    ar_df.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "    ar_df[\"val_num\"] = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "    ar_df = ar_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(ar_df, \"PREDICT_ANOMALY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test environment will now have three readings:  \n",
    "READINGS: the raw data  \n",
    "TRAINING_ANOMALY: data with anomalies flag manually and during the training  \n",
    "PREDICT_ANOMALY: data with anomalies detected by the prediction step  \n",
    "\n",
    "As well as a standardizer and a model for each data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
