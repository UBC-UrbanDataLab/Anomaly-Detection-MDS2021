{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Framework Example\n",
    "\n",
    "This notebook provides a walkthrough of using the anomaly detection framework in a test environment. This test environment was used as UDL's InfluxDB instance was still being setup with SkySpark data during the project. The test environment populates an instance of InfluxDB (created using Docker) with sensor data from `../../data/labelled-skyspark-data/`. The sensor data was manually downloaded from SkySpark and corresponds with five sensors used in Phase 1 model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# influxdb_client is used to populate InfluxDB with the csv data\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the model package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are contained in a sibling folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import model.clean as cl\n",
    "import model.model_trainer as mt\n",
    "import model.model_predict as mp\n",
    "from model.influx_interact import influx_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Local InfluxDB Instance\n",
    "\n",
    "Copy `docker-compose.yml` located in this directory to a local directory. Then run the command `docker-compose up` from this local directory.\n",
    "\n",
    "Go to `http://localhost:8086/` and enter `MDS2021` as user name and `mypassword` to log in to the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Populate InfluxDB with Sensor Data\n",
    "\n",
    "This step will populate InfluxDB with csv files located in `../../data/labelled-skyspark-data/`. These files correspond with the Phase 1 model testing. The code presented in this section is also available in `populate_influx.py`.\n",
    "\n",
    "Note that this step is just for creating the data in this test environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "CSVS_TO_LOAD = [\n",
    "    \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"CEC_compiled_data_5b_updated.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a look up table for sensors and their manually labeled data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOIN_MANUAL_ANOMALIES = True\n",
    "PATH_TO_LABELLED_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "LABELLED_LOOKUP = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\" : \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\" : \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\" : \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\" : \"CEC_compiled_data_5b_updated.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data viewing in notebook\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up InfluxDB connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as setup in docker-compose.yml\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, timeout=999_000)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each csv file and write the data to InfluxDB. This sets up the sensor data in InfluxDB in the READINGS measurement mimicing how SkySpark data exists in InfluxDB. Note that only the tags/field required for anomaly detection are populated.\n",
    "\n",
    "Important note: If the influx write times out, re-run and it should work on the second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing: CEC_compiled_data_1b_updated.csv\n",
      "writing: CEC_compiled_data_2b_updated.csv\n",
      "writing: CEC_compiled_data_3b_updated.csv\n",
      "writing: CEC_compiled_data_4b_updated.csv\n",
      "writing: CEC_compiled_data_5b_updated.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in CSVS_TO_LOAD:\n",
    "\n",
    "    # load and set up dataframes\n",
    "    df = pd.read_csv(PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"])\n",
    "    df.rename(columns={\"Value\": \"val_num\"}, inplace=True)\n",
    "    df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    df.rename(columns={\"Anomaly\": \"AH\"}, inplace=True)\n",
    "    df[\"navName\"] = \"Energy\"\n",
    "    df[\"siteRef\"] = \"Campus Energy Centre\"\n",
    "    df.set_index(\"Datetime\", drop=True, inplace=True)\n",
    "    df = df.drop([\"AH\"], axis=1)\n",
    "\n",
    "    print(\"writing: {}\".format(csv))\n",
    "    # write values\n",
    "    write_api.write(\n",
    "        bucket,\n",
    "        org,\n",
    "        record=df,\n",
    "        data_frame_measurement_name=\"READINGS\",\n",
    "        data_frame_tag_columns=[\"uniqueID\", \"navName\", \"siteRef\"],\n",
    "    )\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `df` object to see what was written to influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     val_num                                    uniqueID navName               siteRef\n",
       "Datetime                                                                                              \n",
       "2020-01-01 07:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:00:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:15:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:30:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val_num</th>\n      <th>uniqueID</th>\n      <th>navName</th>\n      <th>siteRef</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-01 07:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:00:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:15:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:30:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor data has now been written to the InfluxDB READINGS measurement. A screenshot of what this looks like in InfluxDB is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Test Anomaly Detection Model Training\n",
    "\n",
    "This step tests model training. This would be typically run on a selected interval (for example every month) to update the anomaly detection models. A script for model training that can be used with UDL's InfluxDB instance is available in `../code/sensor_training.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_training.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_training.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the option to subset the training data for faster testing. Model training can be completed using the entire sensors record by setting this to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sensor threshold ratios for anomaly detection. In the prediciton stage a 99.5 percentile will be calculated on the loss and saved.  \n",
    "On prediction this percentile value will be loaded.  \n",
    "In each case the threshold_ratio will be multipled with the percentile to get the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_RATIOS = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\": 1.8,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\": 1,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\": 0.23,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\": 0.3,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\": 1.72,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the sequence time step sizes.  \n",
    "They are currently all the same, but can individually be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_SIZES = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing for window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZES = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End time to be used for model training such that data that will be predicted during Step 4 of this test environment is not used in model training In the `sensor_training.py` there is no need to set an end time as the model will train on all available data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = 1613109600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides data removal of manually labelled anomalous data for \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ANOMALOUS = True\n",
    "REMOVE_ANOMALOUS_DATA = [\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths to save the model and standard scaler from the cleaning pipeline and create the InfluxDB client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./test_env_models/\"\n",
    "scaler_path = \"./test_env_standardizers/\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "url = \"http://localhost:8086\"\n",
    "\n",
    "influx_conn = influx_class(\n",
    "    org=org,\n",
    "    url=url,\n",
    "    bucket=bucket,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for model training from the InfluxDB READINGS measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_bucket` object is a dictionary with the name of the sensor as the key and then the value is another dict of data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['Campus Energy Centre Boiler B-1 Exhaust O2', 'Campus Energy Centre Boiler B-1 Gas Pressure', 'Campus Energy Centre Campus HW Main Meter Entering Water Temperature', 'Campus Energy Centre Campus HW Main Meter Flow', 'Campus Energy Centre Campus HW Main Meter Power'])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "main_bucket.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the csvs again to get manual anomalies, in the live implementation this would be a second measument and there would be a read then a join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populating AH with False\n",
    "# TODO\n",
    "for key, df in main_bucket.items():\n",
    "    influx_df = df.copy(deep=True)\n",
    "    if JOIN_MANUAL_ANOMALIES:\n",
    "        csv = LABELLED_LOOKUP[key]\n",
    "        df_with_manual_anomaly = pd.read_csv(\n",
    "                PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"]\n",
    "        )\n",
    "        df_with_manual_anomaly = df_with_manual_anomaly.drop_duplicates()\n",
    "        df_with_manual_anomaly[\"Datetime\"] = pd.to_datetime(\n",
    "            df_with_manual_anomaly[\"Datetime\"], utc=True\n",
    "        )\n",
    "        df = df.merge(\n",
    "            df_with_manual_anomaly[[\"Datetime\", \"Anomaly\"]],\n",
    "            how=\"left\",\n",
    "            left_on=\"DateTime\",\n",
    "            right_on=\"Datetime\",\n",
    "        )\n",
    "        df = df.drop(columns=[\"DateTime\"], axis=1)\n",
    "        df.rename(columns={\"Anomaly\": \"manual_anomaly\"}, inplace=True)\n",
    "        main_bucket[key] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides model training by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Removes anomalous data based on manual_anomaly labels available in the TRAINING_ANOMALY measurement\n",
    "2. Standardizes the values for training and saves the standardizer\n",
    "3. Subsets the data for faster training if specified in the `TESTING` variable\n",
    "4. Sequences the values into windows for the LSTM-ED anomaly detection model\n",
    "5. Fits the LSTM-ED and saves the model \n",
    "6. Writes model training anomaly predictions to the TRAINING_ANOMALY Measurement model_anomaly field in InfluxDB\n",
    "\n",
    "**Note:** 3. only applies to this test environment and would not exist in `sensor_training.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============] - 1s 25ms/step - loss: 0.1577 - val_loss: 0.2311\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1520 - val_loss: 0.2441\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1510 - val_loss: 0.2396\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1496 - val_loss: 0.2394\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1481 - val_loss: 0.2544\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1470 - val_loss: 0.2387\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1439 - val_loss: 0.2227\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1455 - val_loss: 0.2630\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1448 - val_loss: 0.2268\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1453 - val_loss: 0.2197\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1436 - val_loss: 0.2166\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1439 - val_loss: 0.2349\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1418 - val_loss: 0.2275\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1431 - val_loss: 0.2468\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1432 - val_loss: 0.2472\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1397 - val_loss: 0.2375\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1401 - val_loss: 0.2440\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1386 - val_loss: 0.2373\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1382 - val_loss: 0.2304\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1386 - val_loss: 0.2363\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1374 - val_loss: 0.2111\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1411 - val_loss: 0.2445\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1412 - val_loss: 0.2523\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1385 - val_loss: 0.2398\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1378 - val_loss: 0.2413\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1376 - val_loss: 0.2167\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1387 - val_loss: 0.2305\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1362 - val_loss: 0.2401\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1368 - val_loss: 0.2361\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1354 - val_loss: 0.2196\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1350 - val_loss: 0.2481\n",
      "this is the train_mae_loss average\n",
      "[0.25596018]\n",
      "8.95409479175198\n",
      "this is the train_mae_loss average\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "this is the loss percentile\n",
      "8.95409479175198\n",
      "this is the loss percentile\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Entering Water Temperature\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 29ms/step - loss: 0.1127 - val_loss: 0.1081\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0822 - val_loss: 0.0890\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0740 - val_loss: 0.0641\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0676 - val_loss: 0.0621\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0652 - val_loss: 0.0763\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0621 - val_loss: 0.0593\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0603 - val_loss: 0.0621\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0579 - val_loss: 0.0696\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0589 - val_loss: 0.0585\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0571 - val_loss: 0.0574\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0549 - val_loss: 0.0552\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0544 - val_loss: 0.0555\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0530 - val_loss: 0.0595\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0529 - val_loss: 0.0509\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0526 - val_loss: 0.0538\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0532 - val_loss: 0.0549\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0532 - val_loss: 0.0565\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0528 - val_loss: 0.0521\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0516 - val_loss: 0.0497\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0519 - val_loss: 0.0564\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0514 - val_loss: 0.0501\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0506 - val_loss: 0.0538\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0521 - val_loss: 0.0517\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0536 - val_loss: 0.0494\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0505 - val_loss: 0.0515\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0504 - val_loss: 0.0520\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0506 - val_loss: 0.0524\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0516 - val_loss: 0.0482\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0508 - val_loss: 0.0526\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0521 - val_loss: 0.0545\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0522 - val_loss: 0.0497\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.0502 - val_loss: 0.0487\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0503 - val_loss: 0.0521\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0499 - val_loss: 0.0539\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0508 - val_loss: 0.0549\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.0508 - val_loss: 0.0493\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0512 - val_loss: 0.0502\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0499 - val_loss: 0.0544\n",
      "this is the train_mae_loss average\n",
      "[0.08555091]\n",
      "0.2568173482562219\n",
      "this is the train_mae_loss average\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "this is the loss percentile\n",
      "0.2568173482562219\n",
      "this is the loss percentile\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Flow\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 5s 31ms/step - loss: 0.3223 - val_loss: 0.2259\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1576 - val_loss: 0.1744\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1311 - val_loss: 0.0718\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1196 - val_loss: 0.0678\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1077 - val_loss: 0.0715\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1023 - val_loss: 0.0993\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1181 - val_loss: 0.0724\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0997 - val_loss: 0.0874\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1040 - val_loss: 0.0694\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1004 - val_loss: 0.0585\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0983 - val_loss: 0.0634\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0951 - val_loss: 0.1155\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1063 - val_loss: 0.0588\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1012 - val_loss: 0.0566\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0971 - val_loss: 0.0554\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0946 - val_loss: 0.0579\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.0891 - val_loss: 0.0547\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0910 - val_loss: 0.0590\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0960 - val_loss: 0.0741\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0902 - val_loss: 0.0673\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0895 - val_loss: 0.0530\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0899 - val_loss: 0.0528\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0883 - val_loss: 0.0525\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0858 - val_loss: 0.0577\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0879 - val_loss: 0.0996\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0986 - val_loss: 0.0526\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0860 - val_loss: 0.0586\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0862 - val_loss: 0.0526\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0841 - val_loss: 0.0528\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0853 - val_loss: 0.0524\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0824 - val_loss: 0.0839\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0900 - val_loss: 0.0623\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0840 - val_loss: 0.0621\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0828 - val_loss: 0.0615\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0847 - val_loss: 0.0694\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0848 - val_loss: 0.0591\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0822 - val_loss: 0.0543\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0815 - val_loss: 0.0548\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0835 - val_loss: 0.0523\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0837 - val_loss: 0.0622\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0819 - val_loss: 0.0653\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0848 - val_loss: 0.0863\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0821 - val_loss: 0.0527\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0805 - val_loss: 0.0563\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0836 - val_loss: 0.0589\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0802 - val_loss: 0.0540\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0798 - val_loss: 0.0534\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0794 - val_loss: 0.0530\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0806 - val_loss: 0.0597\n",
      "this is the train_mae_loss average\n",
      "[0.06720454]\n",
      "0.22569703179054376\n",
      "this is the train_mae_loss average\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "this is the loss percentile\n",
      "0.22569703179054376\n",
      "this is the loss percentile\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Power\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 6s 31ms/step - loss: 0.2351 - val_loss: 0.2602\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1354 - val_loss: 0.2026\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1242 - val_loss: 0.2090\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1196 - val_loss: 0.1650\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1173 - val_loss: 0.2143\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1126 - val_loss: 0.1811\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1152 - val_loss: 0.2014\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1112 - val_loss: 0.1524\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1109 - val_loss: 0.1706\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.1146 - val_loss: 0.2001\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.1109 - val_loss: 0.1558\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1094 - val_loss: 0.1540\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1084 - val_loss: 0.1513\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1097 - val_loss: 0.1531\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1067 - val_loss: 0.1567\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1097 - val_loss: 0.1504\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1074 - val_loss: 0.1523\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1065 - val_loss: 0.1550\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1086 - val_loss: 0.1759\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.1075 - val_loss: 0.1502\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1061 - val_loss: 0.1644\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1068 - val_loss: 0.1480\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1073 - val_loss: 0.1592\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1041 - val_loss: 0.1512\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.1060 - val_loss: 0.1487\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.1065 - val_loss: 0.1505\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1062 - val_loss: 0.1789\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1077 - val_loss: 0.1466\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1045 - val_loss: 0.1468\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.1056 - val_loss: 0.1521\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1041 - val_loss: 0.1475\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1054 - val_loss: 0.1635\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1032 - val_loss: 0.1458\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.1042 - val_loss: 0.1572\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.1028 - val_loss: 0.1466\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.1044 - val_loss: 0.1544\n",
      "Epoch 37/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1036 - val_loss: 0.1456\n",
      "Epoch 38/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1011 - val_loss: 0.1460\n",
      "Epoch 39/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1012 - val_loss: 0.1471\n",
      "Epoch 40/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1036 - val_loss: 0.1458\n",
      "Epoch 41/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1026 - val_loss: 0.1618\n",
      "Epoch 42/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1019 - val_loss: 0.1461\n",
      "Epoch 43/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1022 - val_loss: 0.1737\n",
      "Epoch 44/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0999 - val_loss: 0.1427\n",
      "Epoch 45/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1021 - val_loss: 0.1417\n",
      "Epoch 46/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1011 - val_loss: 0.1380\n",
      "Epoch 47/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0996 - val_loss: 0.1631\n",
      "Epoch 48/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0992 - val_loss: 0.1462\n",
      "Epoch 49/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0983 - val_loss: 0.1432\n",
      "Epoch 50/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0998 - val_loss: 0.1517\n",
      "Epoch 51/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0984 - val_loss: 0.1615\n",
      "Epoch 52/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0974 - val_loss: 0.1377\n",
      "Epoch 53/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1003 - val_loss: 0.1546\n",
      "Epoch 54/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1015 - val_loss: 0.1710\n",
      "Epoch 55/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0981 - val_loss: 0.1474\n",
      "Epoch 56/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0980 - val_loss: 0.1705\n",
      "Epoch 57/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0977 - val_loss: 0.1552\n",
      "Epoch 58/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0977 - val_loss: 0.1357\n",
      "Epoch 59/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0967 - val_loss: 0.1405\n",
      "Epoch 60/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0953 - val_loss: 0.1646\n",
      "Epoch 61/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0956 - val_loss: 0.1499\n",
      "Epoch 62/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0983 - val_loss: 0.1554\n",
      "Epoch 63/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0982 - val_loss: 0.2172\n",
      "Epoch 64/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0970 - val_loss: 0.1489\n",
      "Epoch 65/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0959 - val_loss: 0.1421\n",
      "Epoch 66/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0959 - val_loss: 0.2600\n",
      "Epoch 67/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0971 - val_loss: 0.1659\n",
      "Epoch 68/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0974 - val_loss: 0.1827\n",
      "this is the train_mae_loss average\n",
      "[0.11624478]\n",
      "0.6263057975359085\n",
      "this is the train_mae_loss average\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "this is the loss percentile\n",
      "0.6263057975359085\n",
      "this is the loss percentile\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    print(\"Training for : {}\".format(key))\n",
    "\n",
    "    import importlib\n",
    "    importlib.reload(mt)\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # creates standardized column for each sensor in main bucket\n",
    "    df[\"Stand_Val\"] = cl.std_val_train(\n",
    "        df[[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    if TESTING:\n",
    "        df = df.tail(30000)\n",
    "\n",
    "    # creates sequences for sliding windows for training\n",
    "    threshold_ratio = THRESHOLD_RATIOS[key]\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = WINDOW_SIZES[key]\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    normal_dict = cl.model_parser(df, x_train, y_train)\n",
    "    mt.fit_models(normal_dict, model_path, threshold_ratio)\n",
    "\n",
    "    # creates sequences for sliding windows for predicting on the train set\n",
    "    x_eval, y_eval = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, 1)\n",
    "    x_eval = np.reshape(x_eval, (x_eval.shape[0], x_eval.shape[1], 1))\n",
    "    timestamps = df[\"Datetime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "    manual_anomaly = df[\"manual_anomaly\"].tail(len(df) - x_train.shape[1]).values\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = loss_percentile * threshold_ratio\n",
    "\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    ar_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_eval,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"model_anomaly\",\n",
    "        manual_anomaly = manual_anomaly\n",
    "    )\n",
    "    ar_df = ar_df[[\"uniqueID\", \"val_num\", \"model_anomaly\", \"manual_anomaly\"]]\n",
    "\n",
    "\n",
    "    influx_conn.write_data(ar_df, \"TRAINING_ANOMALY\", tags=[\"uniqueID\", \"model_anomaly\", \"manual_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following screenshots show InfluxDB with the TRAINING_ANOMALY measurements with the model_anomaly field written from the above process. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `model_anomaly` or `manual_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Test Anomaly Detection Predictions\n",
    "\n",
    "This step tests anomaly predictions and includes reading recent data from InfluxDB (including the window of data required to make predictions), loading previously saved anomaly detection models, running these models on the data to provide predictions, and writing the results back to InfluxDB. This would be typically by completed on a high frequency interval (for example every minute or 5 minutes). A script for anomaly predictions that can be used with UDL's InfluxDB instance is available in `../code/sensor_predict.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_predict.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_predictor.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First setup start and end times for the prediction data set in this testing environment. In `sensor_predict.py` END_TIME would be `now()` and START_TIME would be `now() - 1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TIME FOR TRAINING SET BECOMES PREDICTING'S START TIME\n",
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from InfluxDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket_for_pred = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides predictions by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Standardizes the values for training by loading the standardizer\n",
    "2. Sequences the values into windows for the LSTM-ED and other reshaping for the prediction step\n",
    "3. Creates predictions for the data and returns the prediction object\n",
    "4. Shapes the prediction object and write predictions to the PREDICT_ANOMALY Measurement realtime_anomaly field in InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(96, 5)\n",
      "(132, 5)\n",
      "(296, 5)\n",
      "(1386, 5)\n",
      "(660, 5)\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket_for_pred.items():\n",
    "    main_bucket_for_pred[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket_for_pred[key][[\"Value\"]],\n",
    "        main_bucket_for_pred[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(pred_df, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are now written to InfluxDB and a screenshot of the PREDICT_ANOMALY measurement in InfluxDB is shown below. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `realtime_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test environment will now have three measurements:\n",
    "\n",
    "- READINGS: the raw data  \n",
    "- TRAINING_ANOMALY: data with the `manual_anomaly` tag (if a user has input this) and `model_anomaly` tag generated from model training step\n",
    "- PREDICT_ANOMALY: data with the `realtime_anomaly` tag generated from the prediction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Dashboard\n",
    "\n",
    "A template for the dashboard has been provided in this `create-test-env` directory as `cec_boiler_sensors_(test).json`\n",
    "\n",
    "**To upload the dashboard template:**\n",
    "1) Navigate to the `Dashboards` tab on the left panel of the influxdb user interface  \n",
    "2) Click `Create Dashboard` in the top right  \n",
    "3) Click `Import Dashboard` from the drop down  \n",
    "4) Click then upload `cec_boiler_sensors_(test).json`  \n",
    "5) Click the new dashboard to view, you will have to change the start date to view data (try 2020-12-20 to now)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard allows the user to change the time period viewed, whether the dashboard is continuously updated, and select the `anomaly_type` variable dropdown which will change the view between: `manual` (user entered `manual_anomaly` tag from the `TRAINING_ANOMALY` measurement), `model` (model training predictions from the `model_anomaly` tag from the `TRAINING_ANOMALY` measurement), or realtime (realtime predictions from the `realtime_anomaly` tag from the `PREDICT_ANOMALY` measurement`).\n",
    "\n",
    "Screenshot of the user controls and the full dashboard for the 5 sensors are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5a.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several challenges with the dashboard interface in InfluxDB including:\n",
    "\n",
    "- Coloring is not consistent between True/False labels on graphs and it does not appear possible to change this\n",
    "- It does not appear possible to change the point sizes\n",
    "- anomalies were input as tags as plotting boolean field data did not appear possible (there are workarounds to this)\n",
    "\n",
    "As such, it is recommended to explore Grafana if additional styling/capability is required. It may also be necessary to consider modifing the schema such that anomalies are field values intsead of tag values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Notifications\n",
    "\n",
    "This is done manually within InfluxDB. There may be a way to upload a template but this was not explored. The notification rule works by filtering for any data in the `PREDICT_ANOMALY` measurement that has the `realtime_anomaly` tag = True.\n",
    "\n",
    "The notification functionality was tested at a very high level in this study. Basically tyring to answer the question: can notifications be sent using InfluxDB on predicted anomalous data. The answer is `Yes` but additional investigations on notification settings should be completed.\n",
    "\n",
    "The process involves creating three objects:\n",
    "\n",
    "1. Checks  \n",
    "2. Notification Endpoints  \n",
    "3. Notification Rules  \n",
    "\n",
    "### 1) To Create a Check\n",
    "\n",
    "1. Navigate to the `Alerts` tab on the left panel of the influxdb user interface.\n",
    "2. Click `Create` in the top right  \n",
    "3. Click `Threshold Check` from the drop down  \n",
    "4. Define the query to look like (note that prior to creating this, the data explorer must be set on a timeframe that contains data): \n",
    "\n",
    "![query](./demo_screenshots/step6_1a.png)\n",
    "\n",
    "5. Configure Check as follows: \n",
    "\n",
    "![check](./demo_screenshots/step6_1b.png)\n",
    "\n",
    "6. Click the green check box\n",
    "\n",
    "### 2) To Create an Endpoint\n",
    "1. Create a new slack app and copy the incoming webhook https://api.slack.com/messaging/webhooks#create_a_webhook  \n",
    "2. Click `Notification Endpoints` on the middle banner  \n",
    "3. Click `Create` in the top right  \n",
    "4. Choose `Slack` from the drop down, name the endpoint, and paste your incoming webhook from your slack app and click Create    \n",
    "\n",
    "### 3) To Create a Notification Rule  \n",
    "1. Click `Notification Rules` from the middle banner  \n",
    "2. Click `Create` in the top right  \n",
    "3. Configure the Notification Rule to look like:\n",
    "\n",
    "![rule](./demo_screenshots/step6_2a.png)\n",
    "\n",
    "4. Click `Create Notification Rule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Dashboard/Notification Test\n",
    "\n",
    "Upload data that has been flagged as anomalous to InfluxDB to test the notification system.\n",
    "\n",
    "The test data is set up to have 3 time stamps, now, 5 mins ago, and 10 mins ago. The notification system will only trigger on fresh data.\n",
    "\n",
    "**NOTE:** It was found during testing that notifications were sometimes inconsistent. Additional testing on the notification system would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_num</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>realtime_anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1623740755070619000</th>\n",
       "      <td>140.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623740455070618880</th>\n",
       "      <td>-40.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623740155070621952</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     val_num                                         uniqueID realtime_anomaly\n",
       "DateTime                                                                                      \n",
       "1623740755070619000    140.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1623740455070618880    -40.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1623740155070621952     40.0  Campus Energy Centre Campus HW Main Meter Power            False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateTime = [int(time.time_ns()), int(time.time_ns() - 3e11), int(time.time_ns() - 6e11),]\n",
    "val_num = [140.0, -40.0, 40.0]\n",
    "realtime_anomaly = [\"True\", \"True\", \"False\"]\n",
    "uniqueID = [\"Campus Energy Centre Campus HW Main Meter Power\"] * 3\n",
    "\n",
    "data = {\"DateTime\": DateTime, \"val_num\":val_num, \"uniqueID\":uniqueID, \"realtime_anomaly\": realtime_anomaly}\n",
    "test_realtime = pd.DataFrame(data)\n",
    "test_realtime.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "test_realtime.index.rename(\"DateTime\", inplace=True)\n",
    "test_realtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_conn.write_data(test_realtime, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flagged point will appear in the check's history:\n",
    "\n",
    "![notification](./demo_screenshots/step7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a notification will be pushed to slack:\n",
    "\n",
    "![notification](./demo_screenshots/step7b.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "Re running the prediction step with different thresholds.  \n",
    "This doesnt retrain the model, just runs a prediction based on the new threshold set.  \n",
    "This requires the training to have already been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the time period for the new analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sensor and a new threshold ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Campus Energy Centre Campus HW Main Meter Flow']\n"
     ]
    }
   ],
   "source": [
    "update_data = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : 1.5\n",
    "}\n",
    "print(list(update_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    "    id = list(update_data.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket_for_test = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the measurement you want your threshold experiment to be sent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_name = \"TEST_THRESHOLD_METER_FLOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Campus Energy Centre Campus HW Main Meter Flow\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket_for_test.items():\n",
    "    main_bucket_for_test[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket_for_test[key][[\"Value\"]],\n",
    "        main_bucket_for_test[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "    print(key)\n",
    "\n",
    "    # keeps external packages updated in the notebook\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # sets up sequencing\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "\n",
    "    # gets training loss percentile for threshold setting\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(pred_df, measurement_name, tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "7d2dde933846643c70a0971eba9e216bf0ce928f2ec74020d36b05b86c71e3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}