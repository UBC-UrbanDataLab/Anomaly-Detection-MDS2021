{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Framework Example\n",
    "\n",
    "This notebook provides a walkthrough of using the anomaly detection framework in a test environment. This test environment was used as UDL's InfluxDB instance was still being setup with SkySpark data during the project. The test environment populates an instance of InfluxDB (created using Docker) with sensor data from `../../data/labelled-skyspark-data/`. The sensor data was manually downloaded from SkySpark and corresponds with five sensors used in Phase 1 model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# influxdb_client is used to populate InfluxDB with the csv data\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the model package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are contained in a sibling folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import model.clean as cl\n",
    "import model.model_trainer as mt\n",
    "import model.model_predict as mp\n",
    "from model.influx_interact import influx_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Local InfluxDB Instance\n",
    "\n",
    "Copy `docker-compose.yml` located in this directory to a local directory. Then run the command `docker-compose up` from this local directory.\n",
    "\n",
    "Go to `http://localhost:8086/` and enter `MDS2021` as user name and `mypassword` to log in to the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Populate InfluxDB with Sensor Data\n",
    "\n",
    "This step will populate InfluxDB with csv files located in `../../data/labelled-skyspark-data/`. These files correspond with the Phase 1 model testing. The code presented in this section is also available in `populate_influx.py`.\n",
    "\n",
    "Note that this step is just for creating the data in this test environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "CSVS_TO_LOAD = [\n",
    "    \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"CEC_compiled_data_5b_updated.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a look up table for sensors and their manually labeled data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOIN_MANUAL_ANOMALIES = True\n",
    "PATH_TO_LABELLED_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "LABELLED_LOOKUP = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\" : \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\" : \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\" : \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\" : \"CEC_compiled_data_5b_updated.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data viewing in notebook\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up InfluxDB connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as setup in docker-compose.yml\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, timeout=999_000)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each csv file and write the data to InfluxDB. This sets up the sensor data in InfluxDB in the READINGS measurement mimicing how SkySpark data exists in InfluxDB. Note that only the tags/field required for anomaly detection are populated.\n",
    "\n",
    "Important note: If the influx write times out, re-run and it should work on the second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing: CEC_compiled_data_1b_updated.csv\n",
      "writing: CEC_compiled_data_2b_updated.csv\n",
      "writing: CEC_compiled_data_3b_updated.csv\n",
      "writing: CEC_compiled_data_4b_updated.csv\n",
      "writing: CEC_compiled_data_5b_updated.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in CSVS_TO_LOAD:\n",
    "\n",
    "    # load and set up dataframes\n",
    "    df = pd.read_csv(PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"])\n",
    "    df.rename(columns={\"Value\": \"val_num\"}, inplace=True)\n",
    "    df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    df.rename(columns={\"Anomaly\": \"AH\"}, inplace=True)\n",
    "    df[\"navName\"] = \"Energy\"\n",
    "    df[\"siteRef\"] = \"Campus Energy Centre\"\n",
    "    df.set_index(\"Datetime\", drop=True, inplace=True)\n",
    "    df = df.drop([\"AH\"], axis=1)\n",
    "\n",
    "    print(\"writing: {}\".format(csv))\n",
    "    # write values\n",
    "    write_api.write(\n",
    "        bucket,\n",
    "        org,\n",
    "        record=df,\n",
    "        data_frame_measurement_name=\"READINGS\",\n",
    "        data_frame_tag_columns=[\"uniqueID\", \"navName\", \"siteRef\"],\n",
    "    )\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `df` object to see what was written to influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     val_num                                    uniqueID navName               siteRef\n",
       "Datetime                                                                                              \n",
       "2020-01-01 07:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:00:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:15:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:30:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre\n",
       "2020-01-01 08:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Energy  Campus Energy Centre"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val_num</th>\n      <th>uniqueID</th>\n      <th>navName</th>\n      <th>siteRef</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-01 07:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:00:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:15:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:30:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Energy</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor data has now been written to the InfluxDB READINGS measurement. A screenshot of what this looks like in InfluxDB is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Test Anomaly Detection Model Training\n",
    "\n",
    "This step tests model training. This would be typically run on a selected interval (for example every month) to update the anomaly detection models. A script for model training that can be used with UDL's InfluxDB instance is available in `../code/sensor_training.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_training.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_training.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the option to subset the training data for faster testing. Model training can be completed using the entire sensors record by setting this to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sensor threshold ratios for anomaly detection. In the prediciton stage a 99.5 percentile will be calculated on the loss and saved.  \n",
    "On prediction this percentile value will be loaded.  \n",
    "In each case the threshold_ratio will be multipled with the percentile to get the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_RATIOS = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\": 1.8,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\": 1,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\": 0.23,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\": 0.3,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\": 1.72,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the sequence time step sizes.  \n",
    "They are currently all the same, but can individually be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_SIZES = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing for window sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZES = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End time to be used for model training such that data that will be predicted during Step 4 of this test environment is not used in model training In the `sensor_training.py` there is no need to set an end time as the model will train on all available data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = 1613109600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides data removal of manually labelled anomalous data for \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ANOMALOUS = True\n",
    "REMOVE_ANOMALOUS_DATA = [\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths to save the model and standard scaler from the cleaning pipeline and create the InfluxDB client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./test_env_models/\"\n",
    "scaler_path = \"./test_env_standardizers/\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "url = \"http://localhost:8086\"\n",
    "\n",
    "influx_conn = influx_class(\n",
    "    org=org,\n",
    "    url=url,\n",
    "    bucket=bucket,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for model training from the InfluxDB READINGS measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_bucket` object is a dictionary with the name of the sensor as the key and then the value is another dict of data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['Campus Energy Centre Boiler B-1 Exhaust O2', 'Campus Energy Centre Boiler B-1 Gas Pressure', 'Campus Energy Centre Campus HW Main Meter Entering Water Temperature', 'Campus Energy Centre Campus HW Main Meter Flow', 'Campus Energy Centre Campus HW Main Meter Power'])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "main_bucket.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the `manual_anomaly` column with False for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    df_copy = df.copy(deep=True)\n",
    "    df_copy[\"manual_anomaly\"] = False\n",
    "    main_bucket[key] = df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides model training by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Removes anomalous data based on manual_anomaly labels available in the TRAINING_ANOMALY measurement\n",
    "2. Standardizes the values for training and saves the standardizer\n",
    "3. Subsets the data for faster training if specified in the `TESTING` variable\n",
    "4. Sequences the values into windows for the LSTM-ED anomaly detection model\n",
    "5. Fits the LSTM-ED and saves the model \n",
    "6. Writes model training anomaly predictions to the TRAINING_ANOMALY Measurement model_anomaly field in InfluxDB\n",
    "\n",
    "**Note:** 3. only applies to this test environment and would not exist in `sensor_training.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for : Campus Energy Centre Boiler B-1 Exhaust O2\n",
      "                      DateTime  Value navName                                          ID  manual_anomaly  Stand_Val\n",
      "6766 2020-03-12 00:45:00+00:00   18.4  Energy  Campus Energy Centre Boiler B-1 Exhaust O2           False   0.836355\n",
      "6767 2020-03-12 01:00:00+00:00   18.4  Energy  Campus Energy Centre Boiler B-1 Exhaust O2           False   0.836355\n",
      "6768 2020-03-12 01:15:00+00:00   18.4  Energy  Campus Energy Centre Boiler B-1 Exhaust O2           False   0.836355\n",
      "6769 2020-03-12 01:30:00+00:00   18.4  Energy  Campus Energy Centre Boiler B-1 Exhaust O2           False   0.836355\n",
      "6770 2020-03-12 01:45:00+00:00   18.4  Energy  Campus Energy Centre Boiler B-1 Exhaust O2           False   0.836355\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 29ms/step - loss: 0.2045 - val_loss: 0.0259\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1185 - val_loss: 0.0830\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1498 - val_loss: 0.0773\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1153 - val_loss: 0.1796\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1244 - val_loss: 0.1531\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1031 - val_loss: 0.1750\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.1204 - val_loss: 0.0427\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.0701 - val_loss: 0.1267\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1005 - val_loss: 0.1223\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1071 - val_loss: 0.0279\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0626 - val_loss: 0.0955\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "Training for : Campus Energy Centre Boiler B-1 Gas Pressure\n",
      "                      DateTime       Value navName                                            ID  manual_anomaly  Stand_Val\n",
      "6770 2020-03-17 22:00:00+00:00  136.106689  Energy  Campus Energy Centre Boiler B-1 Gas Pressure           False  -0.199025\n",
      "6771 2020-03-17 22:15:00+00:00  136.106689  Energy  Campus Energy Centre Boiler B-1 Gas Pressure           False  -0.199025\n",
      "6772 2020-03-17 22:30:00+00:00  136.106689  Energy  Campus Energy Centre Boiler B-1 Gas Pressure           False  -0.199025\n",
      "6773 2020-03-17 22:45:00+00:00  136.106689  Energy  Campus Energy Centre Boiler B-1 Gas Pressure           False  -0.199025\n",
      "6774 2020-03-17 23:00:00+00:00  136.106689  Energy  Campus Energy Centre Boiler B-1 Gas Pressure           False  -0.199025\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 30ms/step - loss: 0.2130 - val_loss: 0.2858\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1867 - val_loss: 0.2655\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1614 - val_loss: 0.2416\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1569 - val_loss: 0.2471\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1518 - val_loss: 0.2252\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1545 - val_loss: 0.2368\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1490 - val_loss: 0.2273\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1507 - val_loss: 0.2512\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1495 - val_loss: 0.2583\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1475 - val_loss: 0.2423\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1450 - val_loss: 0.2359\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1448 - val_loss: 0.2297\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1454 - val_loss: 0.2325\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1425 - val_loss: 0.2265\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1441 - val_loss: 0.2276\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Entering Water Temperature\n",
      "                        DateTime      Value navName                                                 ID  manual_anomaly  Stand_Val\n",
      "262708 2020-11-02 17:50:00+00:00  61.180000  Energy  Campus Energy Centre Campus HW Main Meter Ente...           False  -0.580277\n",
      "262709 2020-11-02 17:51:04+00:00  61.719997  Energy  Campus Energy Centre Campus HW Main Meter Ente...           False  -0.509372\n",
      "262710 2020-11-02 17:56:48+00:00  62.239998  Energy  Campus Energy Centre Campus HW Main Meter Ente...           False  -0.441092\n",
      "262711 2020-11-02 18:00:00+00:00  62.449997  Energy  Campus Energy Centre Campus HW Main Meter Ente...           False  -0.413518\n",
      "262712 2020-11-02 18:03:51+00:00  61.949997  Energy  Campus Energy Centre Campus HW Main Meter Ente...           False  -0.479172\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 30ms/step - loss: 0.1140 - val_loss: 0.0839\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0804 - val_loss: 0.0751\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0718 - val_loss: 0.0605\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0698 - val_loss: 0.0874\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0674 - val_loss: 0.0659\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0620 - val_loss: 0.0612\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0608 - val_loss: 0.0701\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0586 - val_loss: 0.0602\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0569 - val_loss: 0.0579\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0550 - val_loss: 0.0553\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0530 - val_loss: 0.0568\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.0521 - val_loss: 0.0596\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0529 - val_loss: 0.0541\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 2s 33ms/step - loss: 0.0537 - val_loss: 0.0560\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0527 - val_loss: 0.0591\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0532 - val_loss: 0.0529\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0531 - val_loss: 0.0567\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0520 - val_loss: 0.0547\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0523 - val_loss: 0.0494\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0514 - val_loss: 0.0546\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0516 - val_loss: 0.0578\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0516 - val_loss: 0.0533\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0525 - val_loss: 0.0484\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 2s 31ms/step - loss: 0.0536 - val_loss: 0.0506\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0512 - val_loss: 0.0491\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 2s 34ms/step - loss: 0.0510 - val_loss: 0.0504\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 2s 33ms/step - loss: 0.0512 - val_loss: 0.0501\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0514 - val_loss: 0.0601\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0513 - val_loss: 0.0487\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0501 - val_loss: 0.0511\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0501 - val_loss: 0.0487\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0507 - val_loss: 0.0505\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0506 - val_loss: 0.0550\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Flow\n",
      "                        DateTime  Value navName                                              ID  manual_anomaly  Stand_Val\n",
      "503727 2021-01-21 04:12:00+00:00  145.0  Energy  Campus Energy Centre Campus HW Main Meter Flow           False   0.479948\n",
      "503728 2021-01-21 04:13:00+00:00  150.0  Energy  Campus Energy Centre Campus HW Main Meter Flow           False   0.555328\n",
      "503729 2021-01-21 04:14:04+00:00  155.0  Energy  Campus Energy Centre Campus HW Main Meter Flow           False   0.630708\n",
      "503730 2021-01-21 04:15:04+00:00  150.0  Energy  Campus Energy Centre Campus HW Main Meter Flow           False   0.555328\n",
      "503731 2021-01-21 04:16:04+00:00  155.0  Energy  Campus Energy Centre Campus HW Main Meter Flow           False   0.630708\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 29ms/step - loss: 0.3236 - val_loss: 0.1610\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1464 - val_loss: 0.0864\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1219 - val_loss: 0.1232\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1285 - val_loss: 0.1271\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.1202 - val_loss: 0.0708\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1068 - val_loss: 0.0639\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0995 - val_loss: 0.0609\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0962 - val_loss: 0.0659\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1004 - val_loss: 0.0627\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1017 - val_loss: 0.0610\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0989 - val_loss: 0.0668\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0943 - val_loss: 0.0577\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0993 - val_loss: 0.0568\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0994 - val_loss: 0.0630\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0954 - val_loss: 0.0580\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.0942 - val_loss: 0.0638\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0905 - val_loss: 0.1033\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0984 - val_loss: 0.0698\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0880 - val_loss: 0.0547\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0868 - val_loss: 0.0540\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0892 - val_loss: 0.0550\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.0868 - val_loss: 0.0549\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0856 - val_loss: 0.0581\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0911 - val_loss: 0.0707\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0984 - val_loss: 0.0625\n",
      "Epoch 26/100\n",
      "57/57 [==============================] - 2s 30ms/step - loss: 0.0879 - val_loss: 0.0526\n",
      "Epoch 27/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0884 - val_loss: 0.0643\n",
      "Epoch 28/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0909 - val_loss: 0.0584\n",
      "Epoch 29/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0868 - val_loss: 0.0541\n",
      "Epoch 30/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.0852 - val_loss: 0.0593\n",
      "Epoch 31/100\n",
      "57/57 [==============================] - 2s 28ms/step - loss: 0.0818 - val_loss: 0.0543\n",
      "Epoch 32/100\n",
      "57/57 [==============================] - 2s 26ms/step - loss: 0.0842 - val_loss: 0.0673\n",
      "Epoch 33/100\n",
      "57/57 [==============================] - 2s 37ms/step - loss: 0.0863 - val_loss: 0.0643\n",
      "Epoch 34/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.0950 - val_loss: 0.0665\n",
      "Epoch 35/100\n",
      "57/57 [==============================] - 1s 26ms/step - loss: 0.0879 - val_loss: 0.0532\n",
      "Epoch 36/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.0857 - val_loss: 0.0555\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Power\n",
      "                        DateTime      Value navName                                               ID  manual_anomaly  Stand_Val\n",
      "282079 2021-01-02 19:43:06+00:00  22.500000  Energy  Campus Energy Centre Campus HW Main Meter Power           False   1.073228\n",
      "282080 2021-01-02 19:44:37+00:00  21.800001  Energy  Campus Energy Centre Campus HW Main Meter Power           False   0.993527\n",
      "282081 2021-01-02 19:45:37+00:00  22.500000  Energy  Campus Energy Centre Campus HW Main Meter Power           False   1.073228\n",
      "282082 2021-01-02 19:46:37+00:00  21.700001  Energy  Campus Energy Centre Campus HW Main Meter Power           False   0.982142\n",
      "282083 2021-01-02 19:47:49+00:00  20.900000  Energy  Campus Energy Centre Campus HW Main Meter Power           False   0.891055\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 5s 35ms/step - loss: 0.2478 - val_loss: 0.2411\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1304 - val_loss: 0.2551\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1204 - val_loss: 0.2826\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1174 - val_loss: 0.2821\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.1182 - val_loss: 0.2130\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1172 - val_loss: 0.1667\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.1128 - val_loss: 0.2246\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 21ms/step - loss: 0.1146 - val_loss: 0.1536\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1116 - val_loss: 0.1599\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1105 - val_loss: 0.1795\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1135 - val_loss: 0.1683\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1103 - val_loss: 0.1596\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1113 - val_loss: 0.1586\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1085 - val_loss: 0.1613\n",
      "Epoch 15/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1080 - val_loss: 0.1482\n",
      "Epoch 16/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1095 - val_loss: 0.1488\n",
      "Epoch 17/100\n",
      "57/57 [==============================] - 2s 29ms/step - loss: 0.1059 - val_loss: 0.1738\n",
      "Epoch 18/100\n",
      "57/57 [==============================] - 1s 24ms/step - loss: 0.1094 - val_loss: 0.1533\n",
      "Epoch 19/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1061 - val_loss: 0.1761\n",
      "Epoch 20/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1054 - val_loss: 0.1657\n",
      "Epoch 21/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1077 - val_loss: 0.1550\n",
      "Epoch 22/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1085 - val_loss: 0.1493\n",
      "Epoch 23/100\n",
      "57/57 [==============================] - 1s 23ms/step - loss: 0.1060 - val_loss: 0.1572\n",
      "Epoch 24/100\n",
      "57/57 [==============================] - 1s 25ms/step - loss: 0.1055 - val_loss: 0.1536\n",
      "Epoch 25/100\n",
      "57/57 [==============================] - 2s 27ms/step - loss: 0.1054 - val_loss: 0.1614\n",
      "/Users/mitch/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/create-test-env\n"
     ]
    }
   ],
   "source": [
    "# delete\n",
    "delete_api = client.delete_api()\n",
    "min_time = influx_read_df[\"_time\"].values[0]\n",
    "max_time = influx_read_df[\"_time\"].values[-1]\n",
    "delete_api.delete(str(min_time)+\"Z\", str(max_time)+\"Z\", '_measurement=\"TRAINING_ANOMALY\"', bucket=bucket, org=org)\n",
    "\n",
    "\n",
    "for key, df in main_bucket.items():\n",
    "    print(\"Training for : {}\".format(key))\n",
    "\n",
    "    # avoid stale imports in notebooks\n",
    "    import importlib\n",
    "    importlib.reload(mt)\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # creates standardized column for each sensor in main bucket\n",
    "    df[\"Stand_Val\"] = cl.std_val_train(\n",
    "        df[[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # train on only data points not flagged manually\n",
    "    df = df[df.manual_anomaly != True]\n",
    "\n",
    "    if TESTING:\n",
    "        df = df.tail(30000)\n",
    "\n",
    "\n",
    "    # creates sequences for sliding windows for training\n",
    "    threshold_ratio = THRESHOLD_RATIOS[key]\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = WINDOW_SIZES[key]\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    normal_dict = cl.model_parser(df, x_train, y_train)\n",
    "    mt.fit_models(normal_dict, model_path, threshold_ratio)\n",
    "\n",
    "    # creates sequences for sliding windows for predicting on the train set\n",
    "    x_eval, y_eval = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, 1)\n",
    "    x_eval = np.reshape(x_eval, (x_eval.shape[0], x_eval.shape[1], 1))\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "    manual_anomaly = df[\"manual_anomaly\"].tail(len(df) - x_train.shape[1]).values\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = loss_percentile * threshold_ratio\n",
    "\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    ar_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_eval,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"model_anomaly\",\n",
    "        manual_anomaly = manual_anomaly\n",
    "    )\n",
    "    ar_df = ar_df[[\"uniqueID\", \"val_num\", \"model_anomaly\", \"manual_anomaly\"]]\n",
    "\n",
    "\n",
    "    influx_conn.write_data(ar_df, \"TRAINING_ANOMALY\", tags=[\"uniqueID\", \"model_anomaly\", \"manual_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following screenshots show InfluxDB with the TRAINING_ANOMALY measurements with the model_anomaly field written from the above process. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `model_anomaly` or `manual_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Test Anomaly Detection Predictions\n",
    "\n",
    "This step tests anomaly predictions and includes reading recent data from InfluxDB (including the window of data required to make predictions), loading previously saved anomaly detection models, running these models on the data to provide predictions, and writing the results back to InfluxDB. This would be typically by completed on a high frequency interval (for example every minute or 5 minutes). A script for anomaly predictions that can be used with UDL's InfluxDB instance is available in `../code/sensor_predict.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_predict.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_predictor.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First setup start and end times for the prediction data set in this testing environment. In `sensor_predict.py` END_TIME would be `now()` and START_TIME would be `now() - 1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TIME FOR TRAINING SET BECOMES PREDICTING'S START TIME\n",
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from InfluxDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket_for_pred = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides predictions by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Standardizes the values for training by loading the standardizer\n",
    "2. Sequences the values into windows for the LSTM-ED and other reshaping for the prediction step\n",
    "3. Creates predictions for the data and returns the prediction object\n",
    "4. Shapes the prediction object and write predictions to the PREDICT_ANOMALY Measurement realtime_anomaly field in InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in main_bucket_for_pred.items():\n",
    "    main_bucket_for_pred[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket_for_pred[key][[\"Value\"]],\n",
    "        main_bucket_for_pred[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # avoid stale imports in notebooks\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(pred_df, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are now written to InfluxDB and a screenshot of the PREDICT_ANOMALY measurement in InfluxDB is shown below. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `realtime_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test environment will now have three measurements:\n",
    "\n",
    "- READINGS: the raw data  \n",
    "- TRAINING_ANOMALY: data with the `manual_anomaly` tag (if a user has input this) and `model_anomaly` tag generated from model training step\n",
    "- PREDICT_ANOMALY: data with the `realtime_anomaly` tag generated from the prediction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Dashboard\n",
    "\n",
    "A template for the dashboard has been provided in this `create-test-env` directory as `cec_boiler_sensors_(test).json`\n",
    "\n",
    "**To upload the dashboard template:**\n",
    "1) Navigate to the `Dashboards` tab on the left panel of the influxdb user interface  \n",
    "2) Click `Create Dashboard` in the top right  \n",
    "3) Click `Import Dashboard` from the drop down  \n",
    "4) Click then upload `cec_boiler_sensors_(test).json`  \n",
    "5) Click the new dashboard to view, you will have to change the start date to view data (try 2020-12-20 to now)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard allows the user to change the time period viewed, whether the dashboard is continuously updated, and select the `anomaly_type` variable dropdown which will change the view between: `manual` (user entered `manual_anomaly` tag from the `TRAINING_ANOMALY` measurement), `model` (model training predictions from the `model_anomaly` tag from the `TRAINING_ANOMALY` measurement), or realtime (realtime predictions from the `realtime_anomaly` tag from the `PREDICT_ANOMALY` measurement`).\n",
    "\n",
    "Screenshot of the user controls and the full dashboard for the 5 sensors are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5a.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several challenges with the dashboard interface in InfluxDB including:\n",
    "\n",
    "- Coloring is not consistent between True/False labels on graphs and it does not appear possible to change this\n",
    "- It does not appear possible to change the point sizes\n",
    "- anomalies were input as tags as plotting boolean field data did not appear possible (there are workarounds to this)\n",
    "\n",
    "As such, it is recommended to explore Grafana if additional styling/capability is required. It may also be necessary to consider modifing the schema such that anomalies are field values intsead of tag values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Notifications\n",
    "\n",
    "This is done manually within InfluxDB. There may be a way to upload a template but this was not explored. The notification rule works by filtering for any data in the `PREDICT_ANOMALY` measurement that has the `realtime_anomaly` tag = True.\n",
    "\n",
    "The notification functionality was tested at a very high level in this study. Basically tyring to answer the question: can notifications be sent using InfluxDB on predicted anomalous data. The answer is `Yes` but additional investigations on notification settings should be completed.\n",
    "\n",
    "The process involves creating three objects:\n",
    "\n",
    "1. Checks  \n",
    "2. Notification Endpoints  \n",
    "3. Notification Rules  \n",
    "\n",
    "### 1) To Create a Check\n",
    "\n",
    "1. Navigate to the `Alerts` tab on the left panel of the influxdb user interface.\n",
    "2. Click `Create` in the top right  \n",
    "3. Click `Threshold Check` from the drop down  \n",
    "4. Define the query to look like (note that prior to creating this, the data explorer must be set on a timeframe that contains data): \n",
    "\n",
    "![query](./demo_screenshots/step6_1a.png)\n",
    "\n",
    "5. Configure Check as follows: \n",
    "\n",
    "![check](./demo_screenshots/step6_1b.png)\n",
    "\n",
    "6. Click the green check box\n",
    "\n",
    "### 2) To Create an Endpoint\n",
    "1. Create a new slack app and copy the incoming webhook https://api.slack.com/messaging/webhooks#create_a_webhook  \n",
    "2. Click `Notification Endpoints` on the middle banner  \n",
    "3. Click `Create` in the top right  \n",
    "4. Choose `Slack` from the drop down, name the endpoint, and paste your incoming webhook from your slack app and click Create    \n",
    "\n",
    "### 3) To Create a Notification Rule  \n",
    "1. Click `Notification Rules` from the middle banner  \n",
    "2. Click `Create` in the top right  \n",
    "3. Configure the Notification Rule to look like:\n",
    "\n",
    "![rule](./demo_screenshots/step6_2a.png)\n",
    "\n",
    "4. Click `Create Notification Rule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Dashboard/Notification Test\n",
    "\n",
    "Upload data that has been flagged as anomalous to InfluxDB to test the notification system.\n",
    "\n",
    "The test data is set up to have 3 time stamps, now, 5 mins ago, and 10 mins ago. The notification system will only trigger on fresh data.\n",
    "\n",
    "**NOTE:** It was found during testing that notifications were sometimes inconsistent. Additional testing on the notification system would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     val_num                                         uniqueID realtime_anomaly\n",
       "DateTime                                                                                      \n",
       "1623909897667094000    140.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1623909597667095040    -40.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1623909297667098112     40.0  Campus Energy Centre Campus HW Main Meter Power            False"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val_num</th>\n      <th>uniqueID</th>\n      <th>realtime_anomaly</th>\n    </tr>\n    <tr>\n      <th>DateTime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1623909897667094000</th>\n      <td>140.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1623909597667095040</th>\n      <td>-40.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1623909297667098112</th>\n      <td>40.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "DateTime = [int(time.time_ns()), int(time.time_ns() - 3e11), int(time.time_ns() - 6e11),]\n",
    "val_num = [140.0, -40.0, 40.0]\n",
    "realtime_anomaly = [\"True\", \"True\", \"False\"]\n",
    "uniqueID = [\"Campus Energy Centre Campus HW Main Meter Power\"] * 3\n",
    "\n",
    "data = {\"DateTime\": DateTime, \"val_num\":val_num, \"uniqueID\":uniqueID, \"realtime_anomaly\": realtime_anomaly}\n",
    "test_realtime = pd.DataFrame(data)\n",
    "test_realtime.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "test_realtime.index.rename(\"DateTime\", inplace=True)\n",
    "test_realtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_conn.write_data(test_realtime, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flagged point will appear in the check's history:\n",
    "\n",
    "![notification](./demo_screenshots/step7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a notification will be pushed to slack:\n",
    "\n",
    "![notification](./demo_screenshots/step7b.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "Re running the prediction step with different thresholds.  \n",
    "This doesnt retrain the model, just runs a prediction based on the new threshold set.  \n",
    "This requires the training to have already been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the time period for the new analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sensor and a new threshold ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Campus Energy Centre Campus HW Main Meter Flow']\n"
     ]
    }
   ],
   "source": [
    "update_data = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : 1.5\n",
    "}\n",
    "print(list(update_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    "    id = list(update_data.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket_for_test = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the measurement you want your threshold experiment to be sent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_name = \"TEST_THRESHOLD_METER_FLOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Campus Energy Centre Campus HW Main Meter Flow\n",
      "                                                           uniqueID  val_num  realtime_anomaly\n",
      "DateTime                                                                                      \n",
      "2021-02-12 06:15:26  Campus Energy Centre Campus HW Main Meter Flow    290.0             False\n",
      "2021-02-12 06:16:26  Campus Energy Centre Campus HW Main Meter Flow    285.0             False\n",
      "2021-02-12 06:17:37  Campus Energy Centre Campus HW Main Meter Flow    280.0             False\n",
      "2021-02-12 06:18:37  Campus Energy Centre Campus HW Main Meter Flow    285.0             False\n",
      "2021-02-12 06:19:38  Campus Energy Centre Campus HW Main Meter Flow    275.0             False\n",
      "Index(['uniqueID', 'val_num', 'realtime_anomaly'], dtype='object')\n",
      "uniqueID             object\n",
      "val_num             float64\n",
      "realtime_anomaly       bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket_for_test.items():\n",
    "    main_bucket_for_test[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket_for_test[key][[\"Value\"]],\n",
    "        main_bucket_for_test[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "    print(key)\n",
    "\n",
    "    # keeps external packages updated in\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # sets up sequencing\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "\n",
    "    # gets training loss percentile for threshold setting\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=\"./test_env_loss_percentiles/\")\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "    \n",
    "    # influx_conn.write_data(pred_df, measurement_name, tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "7d2dde933846643c70a0971eba9e216bf0ce928f2ec74020d36b05b86c71e3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}