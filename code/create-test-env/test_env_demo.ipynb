{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Framework Example\n",
    "\n",
    "This notebook provides a walkthrough of using the anomaly detection framework in a test environment. This test environment was used as UDL's InfluxDB instance was still being setup with SkySpark data during the project. The test environment populates an instance of InfluxDB (created using Docker) with sensor data from `../../data/labelled-skyspark-data/`. The sensor data was manually downloaded from SkySpark and corresponds with five sensors used in Phase 1 model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# influxdb_client is used to populate InfluxDB with the csv data\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the model package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are contained in a sibling folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import model.clean as cl\n",
    "import model.model_trainer as mt\n",
    "import model.model_predict as mp\n",
    "from model.influx_interact import influx_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.random import set_seed\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Local InfluxDB Instance\n",
    "\n",
    "Copy `docker-compose.yml` located in this directory to a local directory. Then run the command `docker-compose up` from this local directory.\n",
    "\n",
    "Go to `http://localhost:8086/` and enter `MDS2021` as user name and `mypassword` to log in to the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Populate InfluxDB with Sensor Data\n",
    "\n",
    "This step will populate InfluxDB with csv files located in `../../data/labelled-skyspark-data/`. These files correspond with the Phase 1 model testing. The code presented in this section is also available in `populate_influx.py`.\n",
    "\n",
    "Note that this step is just for creating the data in this test environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "CSVS_TO_LOAD = [\n",
    "    \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"CEC_compiled_data_5b_updated.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a look up table for sensors and their manually labeled data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOIN_MANUAL_ANOMALIES = True\n",
    "PATH_TO_LABELLED_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "LABELLED_LOOKUP = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\" : \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\" : \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\" : \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\" : \"CEC_compiled_data_5b_updated.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data viewing in notebook\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up InfluxDB connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as setup in docker-compose.yml\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, timeout=999_000)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each csv file and write the data to InfluxDB. This sets up the sensor data in InfluxDB in the READINGS measurement mimicing how SkySpark data exists in InfluxDB. Note that only the tags/field required for anomaly detection are populated.\n",
    "\n",
    "Important note: If the influx write times out, re-run and it should work on the second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "writing: CEC_compiled_data_1b_updated.csv\n",
      "writing: CEC_compiled_data_2b_updated.csv\n",
      "writing: CEC_compiled_data_3b_updated.csv\n",
      "writing: CEC_compiled_data_4b_updated.csv\n",
      "writing: CEC_compiled_data_5b_updated.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in CSVS_TO_LOAD:\n",
    "\n",
    "    # load and set up dataframes\n",
    "    df = pd.read_csv(PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"])\n",
    "    df.rename(columns={\"Value\": \"val_num\"}, inplace=True)\n",
    "    df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    df.rename(columns={\"Anomaly\": \"AH\"}, inplace=True)\n",
    "    df[\"siteRef\"] = \"Campus Energy Centre\"\n",
    "    df.set_index(\"Datetime\", drop=True, inplace=True)\n",
    "    df = df.drop([\"AH\"], axis=1)\n",
    "\n",
    "    print(\"writing: {}\".format(csv))\n",
    "    # write values\n",
    "    write_api.write(\n",
    "        bucket,\n",
    "        org,\n",
    "        record=df,\n",
    "        data_frame_measurement_name=\"READINGS\",\n",
    "        data_frame_tag_columns=[\"uniqueID\", \"navName\", \"siteRef\"],\n",
    "    )\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `df` object to see what was written to influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     val_num                                    uniqueID               siteRef\n",
       "Datetime                                                                                      \n",
       "2020-01-01 07:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Campus Energy Centre\n",
       "2020-01-01 08:00:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Campus Energy Centre\n",
       "2020-01-01 08:15:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Campus Energy Centre\n",
       "2020-01-01 08:30:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Campus Energy Centre\n",
       "2020-01-01 08:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2  Campus Energy Centre"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val_num</th>\n      <th>uniqueID</th>\n      <th>siteRef</th>\n    </tr>\n    <tr>\n      <th>Datetime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-01 07:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:00:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:15:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:30:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n    <tr>\n      <th>2020-01-01 08:45:00</th>\n      <td>2.9</td>\n      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n      <td>Campus Energy Centre</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor data has now been written to the InfluxDB READINGS measurement. A screenshot of what this looks like in InfluxDB is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Test Anomaly Detection Model Training\n",
    "\n",
    "This step tests model training. This would be typically run on a selected interval (for example every month) to update the anomaly detection models. A script for model training that can be used with UDL's InfluxDB instance is available in `../code/sensor_training.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_training.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_training.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the option to subset the training data for faster testing. Model training can be completed using the entire sensors record by setting this to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sensor threshold ratios for anomaly detection. In the prediciton stage a 99.5 percentile will be calculated on the loss and saved.  \n",
    "On prediction this percentile value will be loaded.  \n",
    "In each case the threshold_ratio will be multipled with the percentile to get the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_RATIOS = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\": 1.8,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\": 1,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\": 0.23,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\": 0.3,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\": 1.72,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the sequence time step sizes.  \n",
    "They are currently all the same, but can individually be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP_SIZES = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\":15,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\":15,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\":15,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End time ,as unix time stamp, to be used when reading data for model training such that data that will be predicted during Step 4 of this test environment is not used in model training In the `sensor_training.py` there is no need to set an end time as the model will train on all available data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = 1613109600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides data removal of manually labelled anomalous data for \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths to save the model and standard scaler from the cleaning pipeline and create the InfluxDB client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./test_env_models/\"\n",
    "scaler_path = \"./test_env_standardizers/\"\n",
    "percentile_path = \"./test_env_loss_percentiles/\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "url = \"http://localhost:8086\"\n",
    "\n",
    "influx_conn = influx_class(\n",
    "    org=org,\n",
    "    url=url,\n",
    "    bucket=bucket,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for model training from the InfluxDB READINGS measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_df_split_on_sensors = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_bucket` object is a dictionary with the name of the sensor as the key and then the value is another dict of data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['Campus Energy Centre Boiler B-1 Exhaust O2', 'Campus Energy Centre Boiler B-1 Gas Pressure', 'Campus Energy Centre Campus HW Main Meter Entering Water Temperature', 'Campus Energy Centre Campus HW Main Meter Flow', 'Campus Energy Centre Campus HW Main Meter Power'])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "influx_df_split_on_sensors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the `manual_anomaly` column with False for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in influx_df_split_on_sensors.keys():\n",
    "    df = influx_df_split_on_sensors[key].copy(deep=True)\n",
    "    df[\"manual_anomaly\"] = False\n",
    "    influx_df_split_on_sensors[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides model training by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Removes anomalous data based on manual_anomaly labels available in the TRAINING_ANOMALY measurement\n",
    "2. Standardizes the values for training and saves the standardizer\n",
    "3. Subsets the data for faster training if specified in the `TESTING` variable\n",
    "4. Sequences the values into windows for the LSTM-ED anomaly detection model\n",
    "5. Fits the LSTM-ED and saves the model \n",
    "6. Writes model training anomaly predictions to the TRAINING_ANOMALY Measurement model_anomaly field in InfluxDB\n",
    "\n",
    "**Note:** 3. only applies to this test environment and would not exist in `sensor_training.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training for : Campus Energy Centre Boiler B-1 Exhaust O2\n",
      "                   DateTime  Value                                          ID  manual_anomaly\n",
      "0 2020-01-01 07:45:00+00:00    2.9  Campus Energy Centre Boiler B-1 Exhaust O2           False\n",
      "1 2020-01-01 08:00:00+00:00    2.9  Campus Energy Centre Boiler B-1 Exhaust O2           False\n",
      "2 2020-01-01 08:15:00+00:00    2.9  Campus Energy Centre Boiler B-1 Exhaust O2           False\n",
      "3 2020-01-01 08:30:00+00:00    2.9  Campus Energy Centre Boiler B-1 Exhaust O2           False\n",
      "4 2020-01-01 08:45:00+00:00    2.9  Campus Energy Centre Boiler B-1 Exhaust O2           False\n",
      "_measurement=\"TRAINING_ANOMALY\" AND uniqueID=\"Campus Energy Centre Boiler B-1 Exhaust O2\"\n",
      "Epoch 1/100\n",
      "57/57 [==============================] - 4s 29ms/step - loss: 0.2000 - val_loss: 0.1293\n",
      "Epoch 2/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.1626 - val_loss: 0.0736\n",
      "Epoch 3/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.0883 - val_loss: 0.1461\n",
      "Epoch 4/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.1348 - val_loss: 0.0608\n",
      "Epoch 5/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.0756 - val_loss: 0.1784\n",
      "Epoch 6/100\n",
      "57/57 [==============================] - 1s 17ms/step - loss: 0.1404 - val_loss: 0.0945\n",
      "Epoch 7/100\n",
      "57/57 [==============================] - 1s 18ms/step - loss: 0.0913 - val_loss: 0.0766\n",
      "Epoch 8/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0813 - val_loss: 0.0788\n",
      "Epoch 9/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0968 - val_loss: 0.1060\n",
      "Epoch 10/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0642 - val_loss: 0.1374\n",
      "Epoch 11/100\n",
      "57/57 [==============================] - 1s 19ms/step - loss: 0.0885 - val_loss: 0.0985\n",
      "Epoch 12/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0851 - val_loss: 0.0994\n",
      "Epoch 13/100\n",
      "57/57 [==============================] - 1s 20ms/step - loss: 0.0820 - val_loss: 0.1100\n",
      "Epoch 14/100\n",
      "57/57 [==============================] - 1s 22ms/step - loss: 0.0708 - val_loss: 0.0804\n",
      "this is the hook\n",
      "(29986, 15, 1)\n",
      "this is the hook\n",
      "this is the new stuff\n",
      "0.5422071237381377\n",
      "1\n",
      "0.5422071237381377\n",
      "this is the new stuff\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Length of values (29986) does not match length of index (29985)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b3be38c255bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# predicting and prediction formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     pred_df = mp.make_prediction(\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data_labs/DATA599/w2020-data599-capstone-projects-ubc-udl/code/model/model_predict.py\u001b[0m in \u001b[0;36mmake_prediction\u001b[0;34m(model_id, x_data, time_stamps, threshold, values, model_path, anomaly_type, manual_anomaly)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtest_mae_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtest_score_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_stamps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DateTime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtest_score_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_mae_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtest_score_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtest_score_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manomaly_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_score_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtest_score_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"threshold\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3162\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \"\"\"\n\u001b[1;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.8/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \"\"\"\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (29986) does not match length of index (29985)"
     ]
    }
   ],
   "source": [
    "for key, df in influx_df_split_on_sensors.items():\n",
    "    print(\"Training for : {}\".format(key))\n",
    "    print(df.head())\n",
    "\n",
    "    # Delete data that is about to be written, to prevent duplicaete write\n",
    "    delete_predicate = '_measurement=\"TRAINING_ANOMALY\" AND uniqueID=\"{}\"'.format(key)\n",
    "    print(delete_predicate)\n",
    "    delete_api = client.delete_api()\n",
    "    min_time = df[\"DateTime\"].values[0]\n",
    "    max_time = df[\"DateTime\"].values[-1]\n",
    "    delete_api.delete(\n",
    "        str(min_time) + \"Z\",\n",
    "        str(max_time) + \"Z\",\n",
    "        delete_predicate,\n",
    "        bucket=bucket,\n",
    "        org=org,\n",
    "    )\n",
    "\n",
    "    # avoid stale imports in notebooks\n",
    "    import importlib\n",
    "    importlib.reload(mt)\n",
    "    importlib.reload(mp)\n",
    "    importlib.reload(cl)\n",
    "\n",
    "    # creates standardized column for each sensor in main bucket\n",
    "    df[\"Stand_Val\"] = cl.std_val_train(\n",
    "        df[[\"Value\"]],\n",
    "        influx_df_split_on_sensors[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # train on only data points not flagged manually\n",
    "    df = df[df.manual_anomaly != True]\n",
    "\n",
    "    if TESTING:\n",
    "        df = df.tail(30000)\n",
    "\n",
    "\n",
    "    # creates sequences for sliding windows for training\n",
    "    threshold_ratio = THRESHOLD_RATIOS[key]\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = time_steps\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_eval, y_eval = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, 1)\n",
    "    x_eval = np.reshape(x_eval, (x_eval.shape[0], x_eval.shape[1], 1))\n",
    "\n",
    "    # trains and saves model\n",
    "    normal_dict = cl.model_parser(df, x_train, y_train, x_eval)\n",
    "    mt.fit_models(normal_dict, model_path)\n",
    "\n",
    "    # creates sequences for sliding windows for predicting on the train set\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "    manual_anomaly = df[\"manual_anomaly\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=percentile_path)\n",
    "    threshold = loss_percentile * threshold_ratio\n",
    "    print(\"this is the new stuff\")\n",
    "    print(loss_percentile)\n",
    "    print(threshold_ratio)\n",
    "    print(threshold)\n",
    "    print(\"this is the new stuff\")\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_eval,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"model_anomaly\",\n",
    "        manual_anomaly = manual_anomaly\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"model_anomaly\", \"manual_anomaly\"]]\n",
    "\n",
    "    print(pred_df.groupby(\"model_anomaly\").count())\n",
    "    print(pred_df.groupby(\"manual_anomaly\").count())\n",
    "\n",
    "\n",
    "    influx_conn.write_data(pred_df, \"TRAINING_ANOMALY\", tags=[\"uniqueID\", \"model_anomaly\", \"manual_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the files in the saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Campus Energy Centre Campus HW Main Meter Power\nCampus Energy Centre Boiler B-1 Gas Pressure\nREADME.md\nCampus Energy Centre Campus HW Main Meter Flow\nCampus Energy Centre Boiler B-1 Exhaust O2\nCampus Energy Centre Campus HW Main Meter Entering Water Temperature\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "model_files = [f for f in listdir(model_path) if isfile(join(model_path, f))]\n",
    "for model_file in model_files:\n",
    "    print(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following screenshots show InfluxDB with the TRAINING_ANOMALY measurements with the model_anomaly field written from the above process. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `model_anomaly` or `manual_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Test Anomaly Detection Predictions\n",
    "\n",
    "This step tests anomaly predictions and includes reading recent data from InfluxDB (including the window of data required to make predictions), loading previously saved anomaly detection models, running these models on the data to provide predictions, and writing the results back to InfluxDB. This would be typically by completed on a high frequency interval (for example every minute or 5 minutes). A script for anomaly predictions that can be used with UDL's InfluxDB instance is available in `../code/sensor_predict.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_predict.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_predictor.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First setup start and end times for the prediction data set in this testing environment. In `sensor_predict.py` END_TIME would be `now()` and START_TIME would be `now() - 1d`.  \n",
    "They time stamps are unix format.  \n",
    "The START_TIME here is the END_TIME from the training read.  \n",
    "This uses all the data up until `END_TIME` and then uses past `END_TIME` for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TIME FOR TRAINING SET BECOMES PREDICTING'S START TIME\n",
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from InfluxDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_for_pred = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides predictions by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Standardizes the values for training by loading the standardizer\n",
    "2. Sequences the values into windows for the LSTM-ED and other reshaping for the prediction step\n",
    "3. Creates predictions for the data and returns the prediction object\n",
    "4. Shapes the prediction object and write predictions to the PREDICT_ANOMALY Measurement realtime_anomaly field in InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in dfs_for_pred.items():\n",
    "    dfs_for_pred[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        dfs_for_pred[key][[\"Value\"]],\n",
    "        dfs_for_pred[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # avoid stale imports in notebooks\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=percentile_path)\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(pred_df, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are now written to InfluxDB and a screenshot of the PREDICT_ANOMALY measurement in InfluxDB is shown below. Note that as the anomaly labels are tags, it is best to view the data as a scatter plot with the symbol column as uniqueID and the fill column as `realtime_anomaly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test environment will now have three measurements:\n",
    "\n",
    "- READINGS: the raw data  \n",
    "- TRAINING_ANOMALY: data with the `manual_anomaly` tag (if a user has input this) and `model_anomaly` tag generated from model training step\n",
    "- PREDICT_ANOMALY: data with the `realtime_anomaly` tag generated from the prediction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Dashboard\n",
    "\n",
    "A template for the dashboard has been provided in this `create-test-env` directory as `cec_boiler_sensors_(test).json`\n",
    "\n",
    "**To upload the dashboard template:**\n",
    "1) Navigate to the `Dashboards` tab on the left panel of the influxdb user interface  \n",
    "2) Click `Create Dashboard` in the top right  \n",
    "3) Click `Import Dashboard` from the drop down  \n",
    "4) Click then upload `cec_boiler_sensors_(test).json`  \n",
    "5) Click the new dashboard to view, you will have to change the start date to view data (try 2020-12-20 to now)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashboard allows the user to change the time period viewed, whether the dashboard is continuously updated, and select the `anomaly_type` variable dropdown which will change the view between: `manual` (user entered `manual_anomaly` tag from the `TRAINING_ANOMALY` measurement), `model` (model training predictions from the `model_anomaly` tag from the `TRAINING_ANOMALY` measurement), or realtime (realtime predictions from the `realtime_anomaly` tag from the `PREDICT_ANOMALY` measurement`).\n",
    "\n",
    "Screenshot of the user controls and the full dashboard for the 5 sensors are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5a.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](demo_screenshots/step5b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several challenges with the dashboard interface in InfluxDB including:\n",
    "\n",
    "- Coloring is not consistent between True/False labels on graphs and it does not appear possible to change this\n",
    "- It does not appear possible to change the point sizes\n",
    "- anomalies were input as tags as plotting boolean field data did not appear possible (there are workarounds to this)\n",
    "\n",
    "As such, it is recommended to explore Grafana if additional styling/capability is required. It may also be necessary to consider modifing the schema such that anomalies are field values intsead of tag values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Notifications\n",
    "\n",
    "This is done manually within InfluxDB. There may be a way to upload a template but this was not explored. The notification rule works by filtering for any data in the `PREDICT_ANOMALY` measurement that has the `realtime_anomaly` tag = True.\n",
    "\n",
    "The notification functionality was tested at a very high level in this study. Basically tyring to answer the question: can notifications be sent using InfluxDB on predicted anomalous data. The answer is `Yes` but additional investigations on notification settings should be completed.\n",
    "\n",
    "The process involves creating three objects:\n",
    "\n",
    "1. Checks  \n",
    "2. Notification Endpoints  \n",
    "3. Notification Rules  \n",
    "\n",
    "### 1) To Create a Check\n",
    "\n",
    "1. Navigate to the `Alerts` tab on the left panel of the influxdb user interface.\n",
    "2. Click `Create` in the top right  \n",
    "3. Click `Threshold Check` from the drop down  \n",
    "4. Define the query to look like (note that prior to creating this, the data explorer must be set on a timeframe that contains data): \n",
    "\n",
    "![query](./demo_screenshots/step6_1a.png)\n",
    "\n",
    "5. Configure Check as follows: \n",
    "\n",
    "![check](./demo_screenshots/step6_1b.png)\n",
    "\n",
    "6. Click the green check box\n",
    "\n",
    "### 2) To Create an Endpoint\n",
    "1. Create a new slack app and copy the incoming webhook https://api.slack.com/messaging/webhooks#create_a_webhook  \n",
    "2. Click `Notification Endpoints` on the middle banner  \n",
    "3. Click `Create` in the top right  \n",
    "4. Choose `Slack` from the drop down, name the endpoint, and paste your incoming webhook from your slack app and click Create    \n",
    "\n",
    "### 3) To Create a Notification Rule  \n",
    "1. Click `Notification Rules` from the middle banner  \n",
    "2. Click `Create` in the top right  \n",
    "3. Configure the Notification Rule to look like:\n",
    "\n",
    "![rule](./demo_screenshots/step6_2a.png)\n",
    "\n",
    "4. Click `Create Notification Rule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Dashboard/Notification Test\n",
    "\n",
    "Upload data that has been flagged as anomalous to InfluxDB to test the notification system.\n",
    "\n",
    "The test data is set up to have 3 time stamps, now, 5 mins ago, and 10 mins ago. The notification system will only trigger on fresh data.\n",
    "\n",
    "**NOTE:** It was found during testing that notifications were sometimes inconsistent. Additional testing on the notification system would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     val_num                                         uniqueID realtime_anomaly\n",
       "DateTime                                                                                      \n",
       "1624087256017656000    140.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1624086956017657088    -40.0  Campus Energy Centre Campus HW Main Meter Power             True\n",
       "1624086656017658880     40.0  Campus Energy Centre Campus HW Main Meter Power            False"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val_num</th>\n      <th>uniqueID</th>\n      <th>realtime_anomaly</th>\n    </tr>\n    <tr>\n      <th>DateTime</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1624087256017656000</th>\n      <td>140.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1624086956017657088</th>\n      <td>-40.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1624086656017658880</th>\n      <td>40.0</td>\n      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "DateTime = [int(time.time_ns()), int(time.time_ns() - 3e11), int(time.time_ns() - 6e11),]\n",
    "val_num = [140.0, -40.0, 40.0]\n",
    "realtime_anomaly = [\"True\", \"True\", \"False\"]\n",
    "uniqueID = [\"Campus Energy Centre Campus HW Main Meter Power\"] * 3\n",
    "\n",
    "data = {\"DateTime\": DateTime, \"val_num\":val_num, \"uniqueID\":uniqueID, \"realtime_anomaly\": realtime_anomaly}\n",
    "test_realtime = pd.DataFrame(data)\n",
    "test_realtime.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "test_realtime.index.rename(\"DateTime\", inplace=True)\n",
    "test_realtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_conn.write_data(test_realtime, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flagged point will appear in the check's history:\n",
    "\n",
    "![notification](./demo_screenshots/step7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a notification will be pushed to slack:\n",
    "\n",
    "![notification](./demo_screenshots/step7b.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8\n",
    "Re running the prediction step with different thresholds.  \n",
    "This doesnt retrain the model, just runs a prediction based on the new threshold set.  \n",
    "This requires the training to have already been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the time period for the new analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a sensor and a new threshold ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Campus Energy Centre Campus HW Main Meter Flow']\n"
     ]
    }
   ],
   "source": [
    "update_data = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\" : 1.5\n",
    "}\n",
    "print(list(update_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df_for_pred = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    "    id = list(update_data.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_for_test = cl.split_sensors(influx_read_df_for_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the measurement you want your threshold experiment to be sent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_name = \"TEST_THRESHOLD_METER_FLOW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Campus Energy Centre Campus HW Main Meter Flow\n"
     ]
    }
   ],
   "source": [
    "for key, df in dfs_for_test.items():\n",
    "    dfs_for_test[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        dfs_for_test[key][[\"Value\"]],\n",
    "        dfs_for_test[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "    print(key)\n",
    "\n",
    "    # keeps external packages updated in\n",
    "    import importlib\n",
    "    importlib.reload(mp)\n",
    "\n",
    "    # sets up sequencing\n",
    "    time_steps = TIME_STEP_SIZES[key]\n",
    "    window_size = 1\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"], time_steps, window_size)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    # set up lists for passing to predict\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "    val_nums = df[\"Value\"].tail(len(df) - x_train.shape[1]+1).values\n",
    "\n",
    "    # gets training loss percentile for threshold setting\n",
    "    loss_percentile = cl.load_loss_percentile(key, file_path=percentile_path)\n",
    "    threshold = THRESHOLD_RATIOS[key] * loss_percentile\n",
    "\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred_df = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        val_nums,\n",
    "        model_path,\n",
    "        anomaly_type=\"realtime_anomaly\"\n",
    "    )\n",
    "    pred_df = pred_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(pred_df, measurement_name, tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "7d2dde933846643c70a0971eba9e216bf0ce928f2ec74020d36b05b86c71e3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}