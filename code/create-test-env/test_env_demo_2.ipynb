{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Framework Example\n",
    "\n",
    "This notebook provides a walkthrough of using the anomaly detection framework in a test environment. This test environment was used as UDL's InfluxDB instance was still being setup with SkySpark data during the project. The test environment populates an instance of InfluxDB (created using Docker) with sensor data from `../../data/labelled-skyspark-data/`. The sensor data was manually downloaded from SkySpark and corresponds with five sensors used in Phase 1 model testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# influxdb_client is used to populate InfluxDB with the csv data\n",
    "from influxdb_client import InfluxDBClient\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the model package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are contained in a sibling folder\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import model.clean as cl\n",
    "import model.model_trainer as mt\n",
    "import model.model_predict as mp\n",
    "from model.influx_interact import influx_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create Local InfluxDB Instance\n",
    "\n",
    "Copy `docker-compose.yml` located in this directory to a local directory. Then run the command `docker-compose up` from this local directory. It is recommended to increase the ram available to docker from the default of 2gb to 5gb.\n",
    "\n",
    "Go to `http://localhost:8086/` and enter `MDS2021` as user name and `mypassword` to log in. You will need to create the `MDS2021` bucket if it is not already created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Populate InfluxDB with Sensor Data\n",
    "\n",
    "This step will populate InfluxDB with csv files located in `../../data/labelled-skyspark-data/`. These files correspond with the Phase 1 model testing. The code presented in this section is also available in `populate_influx.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "CSVS_TO_LOAD = [\n",
    "    \"CEC_compiled_data_1b_updated.csv\",\n",
    "    \"CEC_compiled_data_2b_updated.csv\",\n",
    "    \"CEC_compiled_data_3b_updated.csv\",\n",
    "    \"CEC_compiled_data_4b_updated.csv\",\n",
    "    \"CEC_compiled_data_5b_updated.csv\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up InfluxDB connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as setup in docker-compose.yml\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "client = InfluxDBClient(url=\"http://localhost:8086\", token=token, timeout=999_000)\n",
    "write_api = client.write_api(write_options=SYNCHRONOUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read each csv file and write the data to InfluxDB. This sets up the sensor data in InfluxDB in the READINGS measurement mimicing how SkySpark data exists in InfluxDB. Note that only the tags/field required for anomaly detection are populated.\n",
    "\n",
    "Important note: If the influx write times out, re-run and it should work on the second try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing: CEC_compiled_data_1b_updated.csv\n",
      "writing: CEC_compiled_data_2b_updated.csv\n",
      "writing: CEC_compiled_data_3b_updated.csv\n",
      "writing: CEC_compiled_data_4b_updated.csv\n",
      "writing: CEC_compiled_data_5b_updated.csv\n"
     ]
    }
   ],
   "source": [
    "for csv in CSVS_TO_LOAD:\n",
    "\n",
    "    # load and set up dataframes\n",
    "    df = pd.read_csv(PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"])\n",
    "    df.rename(columns={\"Value\": \"val_num\"}, inplace=True)\n",
    "    df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    df.rename(columns={\"Anomaly\": \"AH\"}, inplace=True)\n",
    "    df[\"navName\"] = \"Energy\"\n",
    "    df[\"siteRef\"] = \"Campus Energy Centre\"\n",
    "    df.set_index(\"Datetime\", drop=True, inplace=True)\n",
    "    df = df.drop([\"AH\"], axis=1)\n",
    "\n",
    "    print(\"writing: {}\".format(csv))\n",
    "    # write values\n",
    "    write_api.write(\n",
    "        bucket,\n",
    "        org,\n",
    "        record=df,\n",
    "        data_frame_measurement_name=\"READINGS\",\n",
    "        data_frame_tag_columns=[\"uniqueID\", \"navName\", \"siteRef\"],\n",
    "    )\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `df` object to see what was written to influx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_num</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>navName</th>\n",
       "      <th>siteRef</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-01 07:45:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:00:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:15:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:30:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 08:45:00</th>\n",
       "      <td>2.9</td>\n",
       "      <td>Campus Energy Centre Boiler B-1 Exhaust O2</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Campus Energy Centre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     val_num                                    uniqueID  \\\n",
       "Datetime                                                                   \n",
       "2020-01-01 07:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:00:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:15:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:30:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "2020-01-01 08:45:00      2.9  Campus Energy Centre Boiler B-1 Exhaust O2   \n",
       "\n",
       "                    navName               siteRef  \n",
       "Datetime                                           \n",
       "2020-01-01 07:45:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:00:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:15:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:30:00  Energy  Campus Energy Centre  \n",
       "2020-01-01 08:45:00  Energy  Campus Energy Centre  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensor data has now been written to the InfluxDB READINGS measurement. A screenshot of what this looks like in InfluxDB is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Test Anomaly Detection Model Training\n",
    "\n",
    "This step tests model training. This would be typically run on a selected interval (for example every month) to update the anomaly detection models. A script for model training that can be used with UDL's InfluxDB instance is available in `../code/sensor_training.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_training.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_training.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the option to subset the training data for faster testing. Model training can be completed using the entire sensors record by setting this to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide sensor threshold values for anomaly detection. If no value is set, the model will use the default threshold setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = {\n",
    "    \"Campus Energy Centre Campus HW Main Meter Power\": 0.09,\n",
    "    \"Campus Energy Centre Boiler B-1 Exhaust O2\": 0.019,\n",
    "    \"Campus Energy Centre Boiler B-1 Gas Pressure\": 0.0725,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\": 0.02938,\n",
    "    \"Campus Energy Centre Campus HW Main Meter Flow\": 0.043,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End time to be used for model training such that data that will be predicted during Step 4 of this test environment is not used in model training In the `sensor_training.py` there is no need to set an end time as the model will train on all available data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "END_TIME = 1613109600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code provides data removal of manually labelled anomalous data for \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ANOMALOUS = True\n",
    "REMOVE_ANOMALOUS_DATA = [\n",
    "    \"Campus Energy Centre Campus HW Main Meter Entering Water Temperature\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the paths to save the model and standard scaler from the cleaning pipeline and create the InfluxDB client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./test_env_models/\"\n",
    "scaler_path = \"./test_env_standardizers/\"\n",
    "\n",
    "# setup InfluxDB client\n",
    "token = \"mytoken\"\n",
    "org = \"UBC\"\n",
    "bucket = \"MDS2021\"\n",
    "url = \"http://localhost:8086\"\n",
    "\n",
    "influx_conn = influx_class(\n",
    "    org=org,\n",
    "    url=url,\n",
    "    bucket=bucket,\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data for model training from the InfluxDB READINGS measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_bucket` object is a dictionary with the name of the sensor as the key and then the value is another dict of data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Campus Energy Centre Boiler B-1 Exhaust O2', 'Campus Energy Centre Boiler B-1 Gas Pressure', 'Campus Energy Centre Campus HW Main Meter Entering Water Temperature', 'Campus Energy Centre Campus HW Main Meter Flow', 'Campus Energy Centre Campus HW Main Meter Power'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_bucket.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides model training by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Removes anomalous data based on manual_anomaly labels available in the TRAINING_ANOMALY measurement\n",
    "2. Standardizes the values for training and saves the standardizer\n",
    "3. Subsets the data for faster training if specified in the `TESTING` variable\n",
    "4. Sequences the values into windows for the LSTM-ED anomaly detection model\n",
    "5. Fits the LSTEM-ED and saves the model \n",
    "6. Writes model training anomaly predictions to the TRAINING_ANOMALY Measurement model_anomaly field in InfluxDB\n",
    "\n",
    "**Note:** 3. only applies to this test environment and would not exist in `sensor_training.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for : Campus Energy Centre Boiler B-1 Exhaust O2\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 8s 31ms/step - loss: 9.1460e-04 - val_loss: 1.3933\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 3.1749e-04 - val_loss: 1.4148\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 2.5391e-04 - val_loss: 1.4208\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 1.5189e-04 - val_loss: 1.4214\n",
      "Training for : Campus Energy Centre Boiler B-1 Gas Pressure\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 9s 30ms/step - loss: 0.2761 - val_loss: 0.3481\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.2147 - val_loss: 0.3155\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.2075 - val_loss: 0.3346\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.2049 - val_loss: 0.3429\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2039 - val_loss: 0.3523\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Entering Water Temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.8/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "141/141 [==============================] - 7s 26ms/step - loss: 0.2820 - val_loss: 0.2731\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1802 - val_loss: 0.2310\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 19ms/step - loss: 0.1638 - val_loss: 0.1668\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 20ms/step - loss: 0.1501 - val_loss: 0.1784\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1446 - val_loss: 0.1565\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1390 - val_loss: 0.1207\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1339 - val_loss: 0.1520\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1331 - val_loss: 0.1359\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1283 - val_loss: 0.1309\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Flow\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 8s 27ms/step - loss: 0.3026 - val_loss: 0.1498\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.2723 - val_loss: 0.2159\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.2364 - val_loss: 0.3409\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.2351 - val_loss: 0.4512\n",
      "Training for : Campus Energy Centre Campus HW Main Meter Power\n",
      "Epoch 1/100\n",
      "141/141 [==============================] - 7s 27ms/step - loss: 0.2871 - val_loss: 0.1958\n",
      "Epoch 2/100\n",
      "141/141 [==============================] - 4s 25ms/step - loss: 0.2097 - val_loss: 0.1734\n",
      "Epoch 3/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1950 - val_loss: 0.1721\n",
      "Epoch 4/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1847 - val_loss: 0.1664\n",
      "Epoch 5/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1842 - val_loss: 0.1595\n",
      "Epoch 6/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1814 - val_loss: 0.1585\n",
      "Epoch 7/100\n",
      "141/141 [==============================] - 4s 26ms/step - loss: 0.1821 - val_loss: 0.1506\n",
      "Epoch 8/100\n",
      "141/141 [==============================] - 3s 25ms/step - loss: 0.1778 - val_loss: 0.1489\n",
      "Epoch 9/100\n",
      "141/141 [==============================] - 4s 26ms/step - loss: 0.1766 - val_loss: 0.1440\n",
      "Epoch 10/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1727 - val_loss: 0.1419\n",
      "Epoch 11/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1747 - val_loss: 0.1422\n",
      "Epoch 12/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1680 - val_loss: 0.1376\n",
      "Epoch 13/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1655 - val_loss: 0.1349\n",
      "Epoch 14/100\n",
      "141/141 [==============================] - 4s 27ms/step - loss: 0.1678 - val_loss: 0.1308\n",
      "Epoch 15/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1668 - val_loss: 0.1318\n",
      "Epoch 16/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1671 - val_loss: 0.1308\n",
      "Epoch 17/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1640 - val_loss: 0.1280\n",
      "Epoch 18/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1641 - val_loss: 0.1251\n",
      "Epoch 19/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1616 - val_loss: 0.1250\n",
      "Epoch 20/100\n",
      "141/141 [==============================] - 4s 25ms/step - loss: 0.1592 - val_loss: 0.1250\n",
      "Epoch 21/100\n",
      "141/141 [==============================] - 4s 27ms/step - loss: 0.1596 - val_loss: 0.1229\n",
      "Epoch 22/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1571 - val_loss: 0.1221\n",
      "Epoch 23/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1569 - val_loss: 0.1211\n",
      "Epoch 24/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1564 - val_loss: 0.1210\n",
      "Epoch 25/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1530 - val_loss: 0.1192\n",
      "Epoch 26/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1535 - val_loss: 0.1187\n",
      "Epoch 27/100\n",
      "141/141 [==============================] - 3s 24ms/step - loss: 0.1547 - val_loss: 0.1185\n",
      "Epoch 28/100\n",
      "141/141 [==============================] - 3s 25ms/step - loss: 0.1529 - val_loss: 0.1188\n",
      "Epoch 29/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1524 - val_loss: 0.1177\n",
      "Epoch 30/100\n",
      "141/141 [==============================] - 3s 22ms/step - loss: 0.1512 - val_loss: 0.1170\n",
      "Epoch 31/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1519 - val_loss: 0.1188\n",
      "Epoch 32/100\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 0.1477 - val_loss: 0.1194\n",
      "Epoch 33/100\n",
      "141/141 [==============================] - 3s 23ms/step - loss: 0.1527 - val_loss: 0.1201\n"
     ]
    }
   ],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    print(\"Training for : {}\".format(key))\n",
    "\n",
    "    # removes anomalies to only train on normal data\n",
    "    if REMOVE_ANOMALOUS:\n",
    "        if key in REMOVE_ANOMALOUS_DATA:\n",
    "            PATH_TO_CSVS = \"../../data/labelled-skyspark-data/\"\n",
    "            csv = \"CEC_compiled_data_2b_updated.csv\"\n",
    "            df_with_manual_anomaly = pd.read_csv(\n",
    "                PATH_TO_CSVS + csv, parse_dates=[\"Datetime\"]\n",
    "            )\n",
    "            df_with_manual_anomaly[\"Datetime\"] = pd.to_datetime(\n",
    "                df_with_manual_anomaly[\"Datetime\"], utc=True\n",
    "            )\n",
    "            df = df.merge(\n",
    "                df_with_manual_anomaly[[\"Datetime\", \"Anomaly\"]],\n",
    "                how=\"left\",\n",
    "                left_on=\"DateTime\",\n",
    "                right_on=\"Datetime\",\n",
    "            )\n",
    "            df = df.loc[df[\"Anomaly\"] == False]\n",
    "            df = df.drop(columns=[\"DateTime\"], axis=1)\n",
    "            am_df.rename(columns={\"Anomaly\": \"manual_anomaly\"}, inplace=True)\n",
    "\n",
    "    # creates standardized column for each sensor in main bucket\n",
    "    df[\"Stand_Val\"] = cl.std_val_train(\n",
    "        df[[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    if TESTING:\n",
    "        df = df.tail(5000)\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    x_train, y_train = mt.create_sequences(df[\"Stand_Val\"], df[\"Stand_Val\"])\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "    normal_dict = cl.model_parser(df, x_train, y_train)\n",
    "\n",
    "    threshold = THRESHOLDS[key]\n",
    "    mt.fit_models(normal_dict, model_path, threshold)\n",
    "\n",
    "    # for writing AM to influx\n",
    "    am_df = normal_dict[key][\"train_score_df\"]\n",
    "    am_df.rename(columns={\"anomaly\": \"model_anomaly\"}, inplace=True)\n",
    "    am_df.rename(columns={\"ID\": \"uniqueID\"}, inplace=True)\n",
    "    am_df.rename(columns={\"Datetime\": \"DateTime\"}, inplace=True)\n",
    "    am_df[\"val_num\"] = df[\"Value\"].iloc[x_train.shape[1] :]\n",
    "    # only if it hasnt already been created earlier\n",
    "    if \"manual_anomaly\" not in set(am_df.columns):\n",
    "        am_df[\"manual_anomaly\"] = False\n",
    "    am_df.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "    am_df = am_df[[\"uniqueID\", \"model_anomaly\", \"val_num\", \"manual_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(am_df, \"TRAINING_ANOMALY\", tags=[\"uniqueID\", \"model_anomaly\", \"manual_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following screenshots show InfluxDB with the TRAINING_ANOMALY measurements with the model_anomaly field written from the above process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Test Anomaly Detection Predictions\n",
    "\n",
    "This step tests anomaly predictions and includes reading recent data from InfluxDB (including the window of data required to make predictions), loading previously saved anomaly detection models, running these models on the data to provide predictions, and writing the results back to InfluxDB. This would be typically by completed on a high frequency interval (for example every minute or 5 minutes). A script for anomaly predictions that can be used with UDL's InfluxDB instance is available in `../code/sensor_predict.py`. Code that is only applicable to this test environment or differs from what would exist in `../code/sensor_predict.py` is noted.\n",
    "\n",
    "The code presented in this section is also available in `test_env_scheduled_predictor.py`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First setup start and end times for the prediction data set in this testing environment. In `sensor_predict.py` END_TIME would be `now()` and START_TIME would be `now() - 1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END TIME FOR TRAINING SET BECOMES PREDICTING'S START TIME\n",
    "START_TIME = 1613109600\n",
    "END_TIME = 1613196000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from InfluxDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_read_df = influx_conn.make_query(\n",
    "    location=\"Campus Energy Centre\",\n",
    "    measurement=\"READINGS\",\n",
    "    start=START_TIME,\n",
    "    end=END_TIME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data based on uniqueID into individual sensor dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_bucket = cl.split_sensors(influx_read_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides predictions by iterating over each sensor in `main_bucket` and:\n",
    "\n",
    "1. Standardizes the values for training by loading the standardizer\n",
    "2. Sequences the values into windows for the LSTM-ED and other reshaping for the prediction step\n",
    "3. Creates predictions for the data and returns the prediction object\n",
    "4. Shapes the prediction object and write predictions to the PREDICT_ANOMALY Measurement realtime_anomaly field in InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in main_bucket.items():\n",
    "    main_bucket[key][\"Stand_Val\"] = cl.std_val_predict(\n",
    "        main_bucket[key][[\"Value\"]],\n",
    "        main_bucket[key][\"ID\"].any(),\n",
    "        scaler_path,\n",
    "    )\n",
    "\n",
    "    # creates arrays for sliding windows\n",
    "    x_train, y_train = mt.create_sequences(\n",
    "        main_bucket[key][\"Stand_Val\"], main_bucket[key][\"Stand_Val\"]\n",
    "    )\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    timestamps = df[\"DateTime\"].tail(len(df) - x_train.shape[1]).values\n",
    "    threshold = THRESHOLDS[key]\n",
    "\n",
    "    # predicting and prediction formatting\n",
    "    pred = mp.make_prediction(\n",
    "        key,\n",
    "        x_train,\n",
    "        timestamps,\n",
    "        threshold,\n",
    "        model_path,\n",
    "    )\n",
    "    ar_df = pd.DataFrame.from_dict(pred[\"data\"])\n",
    "\n",
    "    # prep for writing\n",
    "    ar_df.rename(columns={\"anomaly\": \"realtime_anomaly\"}, inplace=True)\n",
    "    ar_df.rename(columns={\"Timestamp\": \"DateTime\"}, inplace=True)\n",
    "    ar_df[\"uniqueID\"] = key\n",
    "    ar_df.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "    ar_df[\"val_num\"] = df[\"Value\"].tail(len(df) - x_train.shape[1]).values\n",
    "    ar_df = ar_df[[\"uniqueID\", \"val_num\", \"realtime_anomaly\"]]\n",
    "\n",
    "    influx_conn.write_data(ar_df, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are now written to InfluxDB and a screenshot of the PREDICT_ANOMALY measurement in InfluxDB is shown below:\n",
    "\n",
    "\n",
    "The test environment will now have three measurements:\n",
    "\n",
    "- READINGS: the raw data  \n",
    "- TRAINING_ANOMALY: data with the manual_anomaly field and model_anomaly field generated from model training step\n",
    "- PREDICT_ANOMALY: data with the realtime_anomaly field generated from the prediction step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Dashboard\n",
    "\n",
    "A template for the dashboard has been provided in this `create-test-env` directory as `cec_boiler_sensors.json`\n",
    "\n",
    "##### To upload the dashboard json:\n",
    "1) Navigate to the `Dashboards` tab on the left panel of the influxdb gui  \n",
    "2) Click `Create Dashboard` in the top right  \n",
    "3) Click `Import Dashboard` from the drop down  \n",
    "4) Click then upload `cec_boiler_sensors.json`  \n",
    "5) Click the new dashboard to view, you will have to change the start date to view data (Try 2020-12-20 to now)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Notifications\n",
    "\n",
    "This involves creating three objects:  \n",
    "1) Checks  \n",
    "2) Notification Endpoints  \n",
    "3) Notification Rules  \n",
    "\n",
    "#### To create Alert Notifications\n",
    "\n",
    "##### To Create a Check\n",
    "1) Navigate to the `Alerts` tab on the left panel of the influxdb gui  \n",
    "2) Click `Create` in the top right  \n",
    "3) Click `Threshold Check` from the drop down  \n",
    "4) Define the query to look like: \n",
    "![query](./demo_screenshots/check_01.png)\n",
    "5) Configure Check as follows: \n",
    "![check](./demo_screenshots/check_02.png)\n",
    "6) Click the green check box\n",
    "\n",
    "##### To Create an Endpoint\n",
    "1) Create a new slack app and copy the incoming webhook https://api.slack.com/messaging/webhooks#create_a_webhook  \n",
    "2) Click `Notification Endpoints` on the middle banner  \n",
    "3) Click `Create` in the top right  \n",
    "4) Choose `Slack` from the drop down, name the endpoint, and paste your incoming webhook from your slack app and click Create    \n",
    "\n",
    "##### To Create a Notification Rule  \n",
    "1) Click `Notification Rules` from the middle banner  \n",
    "2) Click `Create` in the top right  \n",
    "3) Configure the Notification Rule to look like:\n",
    "![rule](./demo_screenshots/rules_01.png)\n",
    "4) Click `Create Notification Rule`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Dashboard/Notification Test\n",
    "\n",
    "Upload test data that has been flagged as anomalous to influxdb to test the notification system\n",
    "\n",
    "The test data is set up to have 3 time stamps, now, 5 mins ago, and 10 mins ago.  \n",
    "The notification system will only trigger on fresh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_num</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>realtime_anomaly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1623727246467240000</th>\n",
       "      <td>140.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623726946467239936</th>\n",
       "      <td>-40.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623726646467241984</th>\n",
       "      <td>40.0</td>\n",
       "      <td>Campus Energy Centre Campus HW Main Meter Power</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     val_num                                         uniqueID  \\\n",
       "DateTime                                                                        \n",
       "1623727246467240000    140.0  Campus Energy Centre Campus HW Main Meter Power   \n",
       "1623726946467239936    -40.0  Campus Energy Centre Campus HW Main Meter Power   \n",
       "1623726646467241984     40.0  Campus Energy Centre Campus HW Main Meter Power   \n",
       "\n",
       "                    realtime_anomaly  \n",
       "DateTime                              \n",
       "1623727246467240000             True  \n",
       "1623726946467239936             True  \n",
       "1623726646467241984            False  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DateTime = [int(time.time_ns()), int(time.time_ns() - 3e11), int(time.time_ns() - 6e11),]\n",
    "val_num = [140.0, -40.0, 40.0]\n",
    "realtime_anomaly = [\"True\", \"True\", \"False\"]\n",
    "uniqueID = [\"Campus Energy Centre Campus HW Main Meter Power\"] * 3\n",
    "\n",
    "data = {\"DateTime\": DateTime, \"val_num\":val_num, \"uniqueID\":uniqueID, \"realtime_anomaly\": realtime_anomaly}\n",
    "test_realtime = pd.DataFrame(data)\n",
    "test_realtime.set_index(\"DateTime\", drop=True, inplace=True)\n",
    "test_realtime.index.rename(\"DateTime\", inplace=True)\n",
    "test_realtime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "influx_conn.write_data(test_realtime, \"PREDICT_ANOMALY\", tags=[\"uniqueID\", \"realtime_anomaly\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flagged point will appear in the check's history:\n",
    "![notification](./demo_screenshots/notification_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a notification will be pushed to your slack:\n",
    "![notification](./demo_screenshots/notification_02.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "7d2dde933846643c70a0971eba9e216bf0ce928f2ec74020d36b05b86c71e3e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
